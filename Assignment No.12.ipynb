{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8696eb13",
   "metadata": {},
   "source": [
    "### Question 1. In what modes should the PdfFileReader() and PdfFileWriter() File objects will be opened?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04916e2f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'For PdfFileReader() , file objects should be opened in rb i.e. read binary mode; whereas for\\nPdfFileWriter() , file objects should be opened in wb i.e. write binary mode.\\n'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''For PdfFileReader() , file objects should be opened in rb i.e. read binary mode; whereas for\n",
    "PdfFileWriter() , file objects should be opened in wb i.e. write binary mode.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b848d1b",
   "metadata": {},
   "source": [
    "### Question 2. From a PdfFileReader object, how do you get a Page object for page 5?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2388d0d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Downloading PyPDF2-2.10.9-py3-none-any.whl (218 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0.0 in c:\\users\\gaurav patil\\anaconda3\\lib\\site-packages (from PyPDF2) (4.1.1)\n",
      "Installing collected packages: PyPDF2\n",
      "Successfully installed PyPDF2-2.10.9\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "612fcc91",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [44]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPyPDF2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PdfFileReader\n\u001b[0;32m      2\u001b[0m pdf_reader \u001b[38;5;241m=\u001b[39m PdfFileReader(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGaurav Patil\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDataScience_interview_master_doc.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m page \u001b[38;5;129;01min\u001b[39;00m pdf_reader\u001b[38;5;241m.\u001b[39mgetNumPages():\n\u001b[0;32m      4\u001b[0m     pdf_reader\u001b[38;5;241m.\u001b[39mgetPage(pageNumber\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m4\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not iterable"
     ]
    }
   ],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "pdf_reader = PdfFileReader(r\"C:\\Users\\Gaurav Patil\\Downloads\\DataScience_interview_master_doc.pdf\")\n",
    "for page in pdf_reader.getNumPages():\n",
    "    pdf_reader.getPage(pageNumber='4') # put page_no = 5 to get a Page object for page 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63b3d22",
   "metadata": {},
   "source": [
    "### Question 3. What PdfFileReader variable stores the number of pages in the PDF document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2a3d3296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "120\n"
     ]
    }
   ],
   "source": [
    "#Example Code:\n",
    "from PyPDF2 import PdfFileReader\n",
    "pdf_reader = PdfFileReader(r\"C:\\Users\\Gaurav Patil\\Downloads\\DataScience_interview_master_doc.pdf\")\n",
    "print(pdf_reader.getNumPages()) # Prints the number of pages in an input PDF document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc8e81ab",
   "metadata": {},
   "source": [
    "### Question 4. If a PdfFileReader object’s PDF is encrypted with the password swordfish, what must you do before you can obtain Page objects from it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "53b0ce57",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Where  you have  used  Hypothesis  Testing  in your Machine  learning  Solution.  \n",
      "Suppose you are working on a machine learning project, for which you want to predict \n",
      "if a set of patients have or not a mortal disease, based on several features on your \n",
      "dataset as blood pressure, heart rate, pulse and others.  \n",
      "It sounds like a serious project, for which you’ll need to trust your model and \n",
      "predictions, right? That’s why you got hundreds of samples  that your local hospital very \n",
      "gently allowed you to collect, given  the importance and the seriousness of the topic. \n",
      "But how do you know if your sample is representative of the whole population? And \n",
      "how can we know how much difference might be reasonable? For example, assume \n",
      "that thanks to some previous studies , we know th at the actual probability for any given \n",
      "patient of not having this particular disease is 99%. Now suppose that our sample says \n",
      "that 95% of the patients don’t have the condition . Well, 4% difference doesn’t sound \n",
      "like a significant difference that may lead us t o SUCH bad modelling, right? It might \n",
      "not be the same, but it kind of sounds like it may be representative. To confirm this, we \n",
      "need to build a better understanding of the theoretical background.  \n",
      "Let’s start by what we know…the real probability of not havi ng the disease:  \n",
      "P (not having the disease) = 99%  \n",
      "Now let’s assume that we find a new group of 100 people , and we test all of them to \n",
      "check if any has this disease we’re studying. Can we be sure that 99 of these folks \n",
      "won’t have the condition ? Maybe, but there ’s also a possibility that none of them has \n",
      "the disease  or even that several may have it. What we have here  is a binomial \n",
      "probability problem. The objective of this story is not to talk about probabilities,  and \n",
      "however, in simple words,  the binomial probabil ity is no more than a given chance of \n",
      "something happening a fixed number of times, given a prior probability for  each \n",
      "independent event.  We can find it by just applying the following equation:  \n",
      " \n",
      "Where:  \n",
      " n = the number of trials (or the number being sampled)  \n",
      " x = the number of successes desired  \n",
      " p = probability of getting a success in one trial  \n",
      " q = 1 — p = the probability of getting a failure in one trial  \n",
      "So if we want to know what is the probability that in our sample of 100 guys, we \n",
      "don’t have any of them infected with the disease, we may just fill in the blanks to \n",
      "find that the probability is of 36.6%. And if we want to know the probability of having \n",
      "99% f olks NOT infected, we fill in the blanks again, to find out it is approximately \n",
      "37.0%. And this sounds reasonable: getting 100 out of 100 not infected doesn’t \n",
      "sound very unlikely if every single case has 99% of not being infected. And in this \n",
      "line, it also  sounds reasonable that having 99 not infected out 100 folks might be a \n",
      "little more likely.  \n",
      "We could keep going and find the probability of having even less people not infected \n",
      "in our sample of 100 people:  \n",
      " P (not infected 98 out of 100) = 18.5%  \n",
      "\n",
      " P (not infe cted 97 out of 100) = 6.0%  \n",
      " P (not infected 96 out of 100) = 1.5%  \n",
      " P (not infected 95 out of 100) = 0.3%  \n",
      "Now, let’s go back to the sample we had from our friendly local hospital, which says \n",
      "that 95% of the guys in our sample are NOT infected by this horrible  mortal disease. \n",
      "Well, even though it might sound like the difference in between 95% and 99% is not \n",
      "relevant, given that we would not be working with a random sample of folks, but \n",
      "instead these guys belong to the same population that we know has a 99% \n",
      "probability of not being infected, we’d be setting a hypothesis that our sample is \n",
      "representative when in reality we would have only 0.3% chance of obtaining a \n",
      "sample with 95 out of 100 people not infected. Therefore we should reject our \n",
      "hypothesis and not pro ceed.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What  kind of statistical  tests  you have  performed  in your ML Application  \n",
      "In statics we have a lot of tests like t -tets, Z -test, anova test, Welch’s test etc. we can’t \n",
      "say in every problem statement and every project we can use same test. No.  \n",
      "It’s depends on our dataset , problem statements  and goal.  we can use and perform.  \n",
      " \n",
      "What  do you understand  by P Value?  And what  is use of it in ML? \n",
      "P-value  helps us determine how likely it is to get a particular result when the null \n",
      "hypothesis  is assumed to be true. It is the probability of getting a sample like ours or \n",
      "more extreme than ours if the null hypothesis is correct. Therefore, if the null \n",
      "hypothesi s is assumed to be true, the p -value gives us an estimate of how “strange” \n",
      "our sample is.   \n",
      "If the p -value is very small (<0.05 is considered generally), then our sample is \n",
      "“strange,” and this means that our assumption that the null hypothesis is correct is  \n",
      "most likely to be false. Thus, we reject it.  \n",
      "When and how is p -value is used?  \n",
      "P-values are often reported whenever you perform a  statistical significance test  (like t -\n",
      "test, chi -square test etc). These tests typically return a computed test statistic and the \n",
      "associated p -value. This reported value is used to establish the statistical significance \n",
      "of the relationships being tested.  \n",
      "So, whenever you see a p -value,  there is an associated statistical test.  \n",
      "That means  there is Hypothesis testing being conducted with a defined Null \n",
      "Hypothesis (H0) and a corresponding Alternate hypothesis (HA).  \n",
      "The p -value reported is used to make a decision on whether the null hypot hesis being \n",
      "tested can be rejected or not.  \n",
      "Let’s understand a little bit more about the null and alternate hypothesis.  \n",
      "Now, how to frame a Null hypothesis in general?  \n",
      "While the null hypothesis itself changes with every statistical test, there is a general \n",
      "principle to frame it:  \n",
      "The null hypothesis assumes there is ‘no effect’ or ‘relationship’ by default . \n",
      "Significance of P -values:  \n",
      " If p > 0.10 : the observed difference is “not significant”  \n",
      " If p ≤ 0.10 : the observed difference is “marginally significant”  \n",
      " If p ≤ 0.05 : the observed difference is “significant”  \n",
      " If p ≤ 0.01 : the observed difference is “highly significant.”  \n",
      " \n",
      "For example : if you are testing if a drug treatment is effective or not, then the null \n",
      "hypothesis will assume there is not difference in outcome between the treated and \n",
      "untreated groups. Likewise, if you are testing if one variable influence another (say, \n",
      "car weight influences the mileage), then null hypothesis will postulate there is no \n",
      "relationship between the two.  \n",
      " \n",
      " \n",
      "Which  type of error  is severe  Error,  Type  1 or Type  2? And why with example.  \n",
      "Ans:  It’s depends,  based  on the researcher  and instructor  Type 1 (false positive) is \n",
      "worse than a  Type 2 (false negative) error. The rationale boils down to the idea that if \n",
      "you stick to the status quo or default assumption, at least you're not making things \n",
      "worse. And in many cases, that's true.   But like so much in statistics, in application it's \n",
      "not really so black or white. The analogy of the defendant is great for teaching the \n",
      "concept, bu t when we try to make it a rule of thumb for which type of error is worse in \n",
      "practice, it falls.  \n",
      " \n",
      "In one instance, the Type I error may have consequences that are less acceptable \n",
      "than those from a Type II error. In another, the Type II error could be less costly than \n",
      "a Type I error. And sometimes, as Dan Smith pointed out in  Significance  a few years \n",
      "back with respect to Six Sigma and quality improvement, \"neither\" is the only  answer \n",
      "to which error is worse:   \n",
      "Most Six Sigma students are going to use the skills they learn in the context of \n",
      "business. In business, whether we cost a company $3 million by suggesting an \n",
      "alternative process when there is nothing wrong with the current  process or we fail to \n",
      "realize $3 million in gains when we should switch to a new process but fail to do so, \n",
      "the end result is the same. The company failed to capture $3 million in additional \n",
      "revenue.   \n",
      "POTENTIAL CONSEQUENC ES \n",
      "Since there's not a clear rule of thumb about whether Type 1 or Type 2 errors are \n",
      "worse, our best option when using data to test a hypothesis is to look very carefully at \n",
      "the fallout that might follow both kinds of errors. Several experts suggest using a table \n",
      "like the one below to deta il the consequences for a Type 1 and a Type 2 error in your \n",
      "particular analysis.   \n",
      " \n",
      " \n",
      "Where  we can use chi square  and have  used  this test anywhere  in your application  \n",
      "A chi -square test is a statistical test used to compare observed results with expected \n",
      "results. The purpose of this test is to determine if a difference between observed data \n",
      "and expected data is due to chance, or if it is due to a relationship between the variables \n",
      "you are studying. Therefore, a chi -square test is an excellent choice to hel p us better \n",
      "understand and interpret the relationship between our two categorical variables.  \n",
      " \n",
      "Use cases example:  \n",
      "A research scholar is interested in the relationship between the placement of students \n",
      "in the statistics department of a reputed University an d their C.G.P.A (their final \n",
      "assessment score).  \n",
      "He obtains the placement records of the past five years from the placement cell \n",
      "database (at random). He records how many students who got placed fell into each of \n",
      "the following C.G.P.A. categories – 9-10, 8 -9, 7-8, 6-7, and below 6.  \n",
      " \n",
      "If there is no relationship between the placement rate and the C.G.P.A., then the \n",
      "placed students should be equally spread across the different C.G.P.A. categories (i.e. \n",
      "there should be similar numbers of placed students in each category).  \n",
      "However, if students having C.G.P.A more than 8 are more likely to get placed, then \n",
      "there would be a large number of placed students in the  higher C.G.P.A. categories as \n",
      "compared to the lower C.G.P.A. categories. In this case, the data collected would \n",
      "make up the observed frequencies.  \n",
      "\n",
      "So the question is, are these frequencies being observed by chance or do they follow \n",
      "some pattern?  \n",
      "Here enter s the chi -square test!  The chi -square test helps us answer the above \n",
      "question by comparing the observed frequencies to the frequencies that we might \n",
      "expect to obtain purely by chance.  \n",
      "Reference: https://www.analyticsvidhya.com/blog/2019/11/what -is-chi-square -test-\n",
      "how-it-works/  \n",
      "Can we use Chi square with Numerical dataset? If yes, give example. If no, give \n",
      "Reason?  \n",
      "What do you understand by ANOVA Testing?  \n",
      "Analysis of variance (ANOVA) is a statistical technique that is used to check if the \n",
      "means of two or more groups are significantly different from each other. ANOVA \n",
      "checks the impact of one or more factors by comparing the means of different samples.  \n",
      "We can  use ANOVA to prove/disprove if all the medication treatments were equally \n",
      "effective or not.  \n",
      "In simpler and general terms, it can be stated that the ANOVA test is used to identify \n",
      "which process, among all the other processes, is better. The fundamental con cept \n",
      "behind the Analysis of Variance is the “ Linear Model ”.  \n",
      " \n",
      "Example of ANOVA  \n",
      "  \n",
      "An example to understand this can be prescribing medicines.   \n",
      "  \n",
      " Suppose, there is a group of patients who are suffering from fever.   \n",
      " \n",
      " They are being given three different medicines that have the same functionality \n",
      "i.e., to cure fever.   \n",
      " \n",
      " To understand the effectiveness of each medicine and choose the best among \n",
      "them, the ANOVA test is used.   \n",
      "  \n",
      "You may wonder that a t -test can also be used instead of using the ANOVA test. You \n",
      "are probably right, but, since t -tests are used to compare only two things, you will have \n",
      "to run multiple t -tests to come up with an outcome. While that is not the case with the  \n",
      "ANOVA test.   \n",
      "  \n",
      "That is why the ANOVA test is also reckoned as an extension of t -test and z -tests.   \n",
      " \n",
      "Types of ANOVA Test  \n",
      "  \n",
      "The ANOVA test is generally done in three ways depending on the number of \n",
      "Independent Variables (IVs) included in the test. Sometimes  the test includes one IV, \n",
      "sometimes it has two IVs, and sometimes the test may include multiple IVs.   \n",
      "  \n",
      "We have three known types of ANOVA test:   \n",
      "  \n",
      "1. One-Way ANOVA  \n",
      "2. Two-Way ANOVA  \n",
      "3. N-Way ANOVA (MANOVA)  \n",
      "Example: Suppose medical researchers want to find the best  diabetes medicine and \n",
      "choose from four medicines. They can choose 20 patients and give them each of the \n",
      "four medicines for four months.   \n",
      "  \n",
      "The researchers can take note of the sugar levels before and after medication for each \n",
      "medicine and the n to understand whether there is a statistically significant difference \n",
      "in the mean results from the medications, they can use one -way ANOVA.   \n",
      "  \n",
      "The type of medicine can be a factor and reduction in sugar level can be considered \n",
      "the response. Researchers can then calculate the p -value and compare if they are \n",
      "lower than the significance level.   \n",
      "  \n",
      "If the results reveal that there is a statistically significant difference in mean sugar level \n",
      "reductions caused by the four medicines, the  post hoc tests  can be run further to \n",
      "determine which medicine led to this result.  \n",
      " \n",
      "Give me a scenario where you can use Z test and T test.  \n",
      "z-tests are used when we have large sample sizes  (n > 30) , whereas t -tests  are most \n",
      "helpful with a smaller sample size (n < 30). Both methods assume a normal distribution \n",
      "of the data, but the z -tests are most useful when the standard deviation is known.  \n",
      "Z-test is the statistical test, used to analyze whether two population means  are different \n",
      "or not when the variances are known and the sample size is large.  \n",
      "  \n",
      "This test statistic is assumed to have a normal distribution, and standard deviation \n",
      "must be known to perform an accurate z -test. \n",
      "  \n",
      "A z-statistic, or z -score, is a number re presenting the value’s relationship to the mean \n",
      "of a group of values, it is measured with population parameters such as population \n",
      "standard deviation and used to validate a hypothesis . \n",
      "  \n",
      "For example, the null hypothesis is “sample mean is the same as the p opulation \n",
      "mean”, and the alternative hypothesis is “the sample mean  is not the same as the \n",
      "population mean”.  \n",
      "T-test: \n",
      "In order to know how significant  the difference between two groups are, a T-test is \n",
      "used ; basically , it tells that difference (measured in means) between two separate \n",
      "groups could have occurred by chance.   \n",
      "  \n",
      "This test assumes to have a normal distribution while based on t -distribution, and \n",
      "population parameters such as mean, or standard deviation are unknown.  \n",
      "  \n",
      "The ratio between the difference between two groups and the difference within the \n",
      "group is known as T -score. Greater is the t -score, more is the difference between \n",
      "groups, and smaller  is the t -score, more similarities are there among groups.   \n",
      "  \n",
      "For example, a t -score value of 2 indicates that the groups are two times as different \n",
      "from each other as they are with each other.   \n",
      "Also, after running t -test, if the larger t -value is obtained , it is highly likely that the \n",
      "outcomes are more repeatable, such that  \n",
      "  \n",
      " A larger t -score states that groups are different  \n",
      " A smaller t -score states that groups are similar.  \n",
      "  \n",
      "Mainly, there are three types of t -test: \n",
      "  \n",
      "1. An Independent Sample t -test, compare the means for two groups.  \n",
      "2. A Paired Sample t -test, compare means from the same group but at different \n",
      "times, such as six months apart.   \n",
      "3. A One Sample t -test, test a mean of a group against the known mean.  \n",
      "What  do you understand  by inferential  Statistics?  \n",
      "· Inferential statistics is work with a random sample of data taken from a population to \n",
      "illustrate and make inferences about the population.  \n",
      "· Inferential statistics are valuable when working with of each member of an entire \n",
      "population is not convenient o r possible.  \n",
      "· It’s help us get to the conclusions and make predictions based on our data.  \n",
      "· Inferential statistics understands the whole population from sample taken from it.  \n",
      "· In Inferential statistics we use a random sample, so we can generalize outcome from \n",
      "the sample to the large population.  \n",
      "· In Inferential statistics, we can calculate the mean, standard deviation, and proportion \n",
      "for our random sample data from population.  \n",
      "The following types of inferential statistics are mostly used and quite easy to interpret:  \n",
      "· Conditional Probability  \n",
      "· Probability Distribution and Distribution function  \n",
      "· Probability  \n",
      "· Regression Analysis  \n",
      "· Central Limit Theorem  \n",
      "· Hypothesis Testing  \n",
      "· T- Test \n",
      "· Z- Test \n",
      "· Sampling Distribution  \n",
      "· Chi -square test  \n",
      "· Confidence Interval  \n",
      "· ANOVA (Analysis of variance)  \n",
      " \n",
      "When  you are trying  to calculate  Std Deviation  or Variance,  why you used  N-1 in \n",
      "Denominator?  \n",
      " \n",
      "The n -1 equation is used in the common situation where you are analyzing  a sample \n",
      "of data and wish to make more general conclusions. The SD computed this way (with \n",
      "n-1 in the denominator) is your best guess for the value of the SD in the overall \n",
      "population.  \n",
      "If you simply want to quantify the variation in a particular set of da ta, and don't plan to \n",
      "extrapolate to make wider conclusions, then you can compute the SD using n in the \n",
      "denominator. The resulting SD is the SD of those particular values. It makes no sense \n",
      "to compute the SD this way if you want to estimate the SD of the p opulation from which \n",
      "those points were drawn. It only makes sense to use n in the denominator when there \n",
      "is no sampling from a population, there is no desire to make general conclusions.   \n",
      " \n",
      "What do you understand by right skewness, Give example?  \n",
      "Ans:   skewness is the measure of how much the probability distribution of a random \n",
      "variable deviates from the  normal distribution . Now, you might be thinking – why am I \n",
      "talking about normal distribution here?  \n",
      "Well, the normal distribution is the probability distribution without any skewness. You \n",
      "can look at the image below which shows symmetri cal distribution that’s basically a \n",
      "normal distribution and you can see that it is symmetrical on both sides of the dashed \n",
      "line. Apart from this, there are two types of skewness:  \n",
      " \n",
      "The probability distribution with its tail on the right side is a positivel y skewed \n",
      "distribution and the one with its tail on the left side is a negatively skewed distribution. \n",
      "If you’re finding the above figures confusing, that’s alright. We’ll understand this in \n",
      "more detail later.  \n",
      "What is difference between Normal distribution and Std Normal Distribution and \n",
      "Uniform Distribution?  \n",
      "The only thing similar about the two is that they are both continuous distributions with \n",
      "two parameters. Differences include:  \n",
      "1. Normal has infinite support, uniform has finite support  \n",
      "2. Normal has a single most likely value, uniform has every allowable value \n",
      "equally likely  \n",
      "3. Uniform has a piecewise constant density, normal has a continuous bell -shaped \n",
      "density  \n",
      "4. Normal distributions arise from the central limit theorem, uniforms do not.  \n",
      "What  is different  kind of Probabilistic  distributions  you heard  of? \n",
      "Probability:  Simply put, probability is an intuitive concept. We use it on a daily basis \n",
      "without necessarily realising that we are speaking and applying probability to work.  \n",
      "Life is full of uncertainties. We don’t know the outcomes of a particular situation until it \n",
      "happens.  Will it rain today? Will I pass the next math test? Will my favourite team win \n",
      "the toss? Will I get a promotion in next 6 months? All these questions are examples of \n",
      "uncertain situations we live in. Let us map them to few common terminologies which \n",
      "we will use going forward.  \n",
      "\n",
      "Experiment  – are the uncertain situations, which could have multiple outcomes. \n",
      "Whether it rains on a daily basis is an experiment.  \n",
      "Outcome  is the result of a single trial. So, if it rains today, the outcome of today’s trial \n",
      "from the experiment is “It rained”  \n",
      "Event  is one or more outcome from an experiment. “It rained” is one of the possible \n",
      "events for this experiment.  \n",
      "Probability  is a measure of how likely an event is. So, if it is 60% chance that it will rain \n",
      "tomorrow, the probability of Outcome “it rained” for tomorrow is 0.6  \n",
      "Types of Distributions  \n",
      "1. Bernoulli Distribution  \n",
      "2. Uniform Distribution  \n",
      "3. Binomial Distribution  \n",
      "4. Normal Distribution  \n",
      "5. Poisson Distribution  \n",
      "6. Exponential Distribution  \n",
      " \n",
      "What  do you understand  by symmetric  dataset?  \n",
      "If the data are symmetric,  they have about the same shape on either side of the middle . \n",
      "In other words, if you fold the histogram in half, it looks about the same on both sides. \n",
      "Below  figure shows an example of symmetric data. With symmetric data, the mean \n",
      "and median are close together.  \n",
      " \n",
      " \n",
      "\n",
      "In your last project,  were  you using  symmetric  data or Asymmetric  Data,  if its \n",
      "asymmetric,  what  kind of EDA  \n",
      "you have  performed?  \n",
      "Ans:  Recently  I have  done  one Chatbot  project,  here  I have  dataset  in the from  of \n",
      "CSV.  During  the EDA  we have  some  lots of steps  because  our dataset  is a \n",
      "imbalanced  and in our dataset  we have  lots of outliers.    \n",
      "1. Here,  we are plotting  a Box plot and scatter  plot for checking  a outlier.  \n",
      "2. We are plotting  a count  plot for checking  a how many  total values  \n",
      "available  in our every  features.    \n",
      "3. As we know  our dataset  is imbalanced  so here  we are used  a under  \n",
      "sampling  for handling  a imbalanced  datasets.  \n",
      "4. Before  handling  a imbalanced  data and outliers  we are getting  43%  \n",
      "accuracy  but after  handling  this we are getting  83%  using  DistilBERT.  \n",
      " \n",
      "Can you please tell me formula for skewness?  \n",
      "Skewness = (3 * (Mean – Median))/Standard Deviation  \n",
      " \n",
      "Have you applied student T distribution Anywhere?  \n",
      "Student T distribution is a type of normal distribution wherein it can be used for smaller \n",
      "sample sizes and is approximately normal distributed.  \n",
      "I have used it in a situation where I was supposed to measure the average of salary of \n",
      "just 10 people which was  very low. But I found that there is some bell curve kind of a \n",
      "structure approximately. So to measure the average of salary I used T distribution  \n",
      " \n",
      "What do you understand by statistical analysis of data, Give me scenario where you \n",
      "have used statistical analysis in last projects?  \n",
      "In one of my projects I had a dataset wherein there were lot of columns. At that time \n",
      "as from domain I made an assumption that these two features looks same so I looked \n",
      "up for the correlation between the columns vs the dependent column and it was \n",
      "multicollinear so I removed one of the columns. So this helped me in feature selection \n",
      "step. Similarly I have used Chi square based approach also in one of my projects. I \n",
      "use describe function in most of my projects to get to know about t he data better.  \n",
      " \n",
      "Can you please tell me criterion to apply binomial distribution, with example?  \n",
      "The criteria are :  \n",
      "The observations must be independent of each other,  \n",
      "The number of observations must be fixed,  \n",
      "The probability of success is same for eac h outcome  \n",
      "A real time example is Lottery ticket where there is only two ways either you reach \n",
      "success or failure  \n",
      " \n",
      " \n",
      "lets suppose I have appeared in 3 interviews, what is the probability that I am able to \n",
      "crack at least 1 interview?  \n",
      "n(S) = 3  \n",
      "P(A<=1) =  1 – P(A=1) * P(A=2) * P(A=3)  \n",
      "       =   1 – ½ * ½ * ½ = 1 – 1/8 = 7/8    \n",
      " \n",
      "Explain Gaussian Distribution in your own way.  \n",
      "Gaussian distribution is also called as normal distribution which has a bell shaped \n",
      "curve structured data distribution. This is denoted as  \n",
      "N(mu, sigma**2)  \n",
      "Here always the skewness and kurtosis is 0  \n",
      " \n",
      "What do you understand by 1st, 2nd and 3rd Standard Deviation from Mean?  \n",
      "According to Empirical Formula,  \n",
      "If the data is normally distributed  \n",
      "The data between the mean and 1st standard deviatio n is 68% approx.  \n",
      "The data between the mean and 2nd standard deviation is 95% approx.  \n",
      "The data between the mean and 3rd standard deviation is 99.7% approx.   \n",
      "If not normal , then we can use Chebyshev’s inequality to find how the data is \n",
      "distributed between mea n and 1st, 2nd and 3rd deviation.  \n",
      " \n",
      " \n",
      " \n",
      "What do you understand by variance in data in simple words?  \n",
      " \n",
      "Variance is spread of dataset. It is a statistical measure which will say how the data is \n",
      "distributed. It shows how much a data is deviated from the mean.  It is denoted by \n",
      "sigma  \n",
      " \n",
      "Formula: Variance = (Summation of(X –mu)**2)/N  \n",
      " \n",
      "X = Data value  \n",
      "mu = mean  \n",
      "N = Total Population  \n",
      " If variance of dataset is too high, in that case How you will be able to handle it or \n",
      "decrease it?  \n",
      " Ensemble models  \n",
      " Increasing the train dataset size  \n",
      " Hyperparameter Tuning  \n",
      " \n",
      "Explain the relationship between Variance and Bias.  \n",
      "Bias tells us the difference between the average difference between the actual value \n",
      "and predicted value and Variance says how the data is spread. Variance and Bias are \n",
      "inversely proportional which means if bias increase variance will decrease or vice \n",
      "versa.  \n",
      " \n",
      " \n",
      " \n",
      "Overfitting – Low Bias High Variance  \n",
      "Underfitting – High Bias High Variance  \n",
      "Perfect – Low Bias, Low Variance   \n",
      " \n",
      "What do you understand by Z Value given in Z Table?  \n",
      "A Z Value in a Z Table will give us how much percentage of data is available to t he left \n",
      "of the given Z score  \n",
      "For example for a specific z score 1.25, 89.44% of value lies behind the given z score \n",
      "(left of it)  \n",
      " \n",
      "Do you know a Standard Normal Distribution Formula?  \n",
      " \n",
      "To convert a data into standard normal distributed data the followin g formula can be \n",
      "used:  \n",
      "z = (x -mean) / standard deviation.   \n",
      " \n",
      "\n",
      "Can you please explain critical region in your way?  \n",
      "It is a region we use in hypothesis testing. We used make a null hypothesis and if the \n",
      "statistic tests like T test Anova test outputs a valu e in this critical region then we will \n",
      "reject the null hypothesis and accept the alternate hypothesis. So it is also called the \n",
      "rejection region.  \n",
      " \n",
      "Have you used AB testing in your project So far? If yes, Explain. If not, Tell me about \n",
      "AB testing.  \n",
      "If yes:  \n",
      "Yes we have used a/b testing in our data science project for Analyzing the Results of \n",
      "two models. It is one of the most effective methods in making conclusions about any \n",
      "hypothesis one may have. We create the A and B version of our model and calculate \n",
      "the success rate of the that based on the comparison  \n",
      " \n",
      "Tell me about AB testing  \n",
      " \n",
      "A/B testing is a basic randomized control experiment. It is a way to compare the two \n",
      "versions of a variable to find out which performs better.  \n",
      " \n",
      "Or  \n",
      " \n",
      "(definition from wikipedia)  \n",
      "A/B testing is a method of comparing two versions of a product or app against each \n",
      "other to determine which one performs better. A/B testing is essentially an experiment \n",
      "where two or more variants of a product are shown to users at random, and statistical \n",
      "analysis is used to determine which variation performs better.  \n",
      " \n",
      "For instance, let’s say you own a comp any and want to increase the sales of your \n",
      "product. Here, either you can use random experiments, or you can apply scientific and \n",
      "statistical methods. A/B testing is one of the most prominent and widely used statistical \n",
      "tools.  \n",
      " \n",
      "In the above scenario, you m ay divide the products into two parts – A and B. Here A \n",
      "will remain unchanged while you make significant changes in B’s packaging. Now, on \n",
      "the basis of the response from customer groups who used A and B respectively, you \n",
      "try to decide which is performing b etter.  \n",
      " \n",
      "Can we use the Alternate hypothesis as a null Hypothesis?  \n",
      "Hypothesis is a statement, assumption or claim about the value of the parameter \n",
      "(mean, variance, median etc.  \n",
      "Like, if we make a statement that “Dhoni is the best Indian Captain ever.” Th is is an \n",
      "assumption that we are making based on the average wins and loses team had under \n",
      "his captaincy.  We can test this statement based on all the match data.  \n",
      "Null Hypothesis  \n",
      "The null hypothesis is the hypothesis to be tested for possible rejection unde r the \n",
      "assumption that it is true. The concept of the null is similar to innocent until proven \n",
      "guilty We assume innocence until we have enough evidence to prove that a suspect \n",
      "is guilty.  \n",
      "It is denoted by H0.  \n",
      "Alternate Hypothesis  \n",
      "The alternative hypothesis complements the Null hypothesis. It is opposite of the null \n",
      "hypothesis such that both Alternate and null hypothesis together cover all the possible \n",
      "values of the population parameter.  \n",
      "It is denoted by H1.  \n",
      "Let’s understand this wi th an example:  \n",
      "\n",
      "A soap company claims that it’s product kills on an average 99% of the germs. To test \n",
      "the claim of this company we will formulate the null and alternate hypothesis.  \n",
      "Null Hypothesis(H0): Average =99%  \n",
      "Alternate Hypothesis(H1): Average is not e qual to 99%.  \n",
      " Note: The thumb rule is that a statement containing equality is the null hypothesis.  \n",
      " Hypothesis Testing  \n",
      "When we test a hypothesis, we assume the null hypothesis to be true until there is \n",
      "sufficient evidence in the sample to prove it false. In that case we reject the null \n",
      "hypothesis and support the alternate hypothesis.  \n",
      "If the sample fails to provide sufficie nt evidence for us to reject the null hypothesis, we \n",
      "cannot say that the null hypothesis is true because it is based on just the sample data. \n",
      "For saying the null hypothesis is true we will have to study the whole population data.  \n",
      "So the main question is: C an we use the Alternate hypothesis as a null Hypothesis?  \n",
      "No, We can’t use it based on the above explanation. The alternate hypothesis  is the \n",
      "opposite of the null hypothesis.  \n",
      " \n",
      "Can you please explain the confusion matrix for more than 2 variables?  \n",
      " \n",
      "Confusion matrix is a performance measurement for machine learning classification \n",
      "problems where output can be two or more classes.  \n",
      " \n",
      "For 2 variable s: \n",
      "It is a table with 4  different combinations  of predicted and actual values  \n",
      " \n",
      " \n",
      "It is extremely useful for measuring Recall, Precision, Specificity, Accuracy, and most \n",
      "importantly AUC -ROC curves.  \n",
      "Let’s understand TP, FP, FN, TN in terms of pregnancy analogy.  \n",
      " \n",
      " \n",
      " \n",
      "Confusion matrix for a 3 class classification:  \n",
      "Let’s try to answer the above question with a popular dataset – IRIS DATASET.  \n",
      "The dataset has 3 flowers as outputs or classes, Versicolor, Virginia, Setosa.  \n",
      "  \n",
      "\n",
      " \n",
      "Source: Google  \n",
      "With the help of petal length, pe tal width, sepal length, sepal width the model has to \n",
      "classify the given instance as Versicolor or Virginia or Setosa flower.  \n",
      "Let’s apply a classifier model here: decision Tree classifier is applied on the above \n",
      "dataset. The dataset has 3 classes hence we get a 3 X 3 confusion matrix.  \n",
      "But how to know TP, TN, FP, FN values !!!!!  \n",
      " \n",
      "In the multi -class classification problem, we won’t get TP, TN, FP, FN values directly \n",
      "as in the binary classification problem. We need to calculate for each class.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "How to calcu late FN, FP, TN, TP :  \n",
      "FN: The False -negative value for a class will be the sum of values of corresponding \n",
      "rows except for the TP value.  \n",
      "FP: The False -positive value for a class will be the sum of values of the corresponding \n",
      "column except for the TP value.  \n",
      "TN: The True Negative value for a class will be the sum of values of all columns and \n",
      "rows except the values of that class that we are calculating the values for.  \n",
      "TP: The True positive value is where the actual value and predicted value are the \n",
      "same.  \n",
      "The co nfusion matrix for the IRIS dataset is as below:  \n",
      " \n",
      "\n",
      " \n",
      " \n",
      "1.Let us calculate the TP, TN, FP, FN values for the class Setosa using the Above \n",
      "tricks:  \n",
      "TP: The actual value and predicted value should be the same. So concerning the \n",
      "Setosa class, the value of cell 1 is the TP value.  \n",
      "FN: The sum of values of corresponding rows except the TP value  \n",
      "FN = (cell 2 + cell3)  \n",
      "= (0 + 0)  \n",
      "= 0 \n",
      "FP : The sum of values of the corresponding column except the TP value.  \n",
      "FP = (cell 4 + cell 7)  \n",
      "= (0 + 0)  \n",
      "= 0 \n",
      "TN: The sum of values of all columns and rows except the values of that class that we \n",
      "are calculating the values for.  \n",
      "TN = (cell 5 + cell 6 + cell 8 + cell 9)  \n",
      "= 17 + 1 +0 + 11 \n",
      "= 29 \n",
      "\n",
      "Similarly, for Versicolor class the values/ metrics are calculated as below:  \n",
      "TP : 17 (cell 5)  \n",
      "FN : 0 + 1 = 1 (cell 4 +cell 6)  \n",
      "FP : 0 + 0 = 0 (cell 2 + cell 8)  \n",
      "TN : 16 +0 +0 + 11 =27 (cell 1 + cell 3 + cell 7 + cell 9).  \n",
      "I hope the concept is clear a nd you can try for the Virginia class.  \n",
      " \n",
      "Give me an example of False Negative From this interview?  \n",
      "A false negative error, or false negative, is a test result which wrongly indicates that a \n",
      "condition does not hold. For example, when a pregnancy test ind icates a woman is not \n",
      "pregnant , but she is, or when a person guilty of a crime is acquitted, these are false \n",
      "negatives.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What do you understand by Precision, Recall and F1 Score with example?  \n",
      "Confusion Matrix  \n",
      "A typical confusion matrix looks like the figure shown.  \n",
      " \n",
      "Where the terms have the meaning:  \n",
      "True Positive(TP):  A result that was predicted as positive by the classification model \n",
      "and also is positive  \n",
      " \n",
      "\n",
      "True Negative(TN):  A result that was predicte d as negative by the classification model \n",
      "and also is negative  \n",
      " \n",
      "False Positive(FP):  A result that was predicted as positive by the classification model \n",
      "but actually is negative  \n",
      " \n",
      "False Negative(FN):  A result that was predicted as negative by the c lassification model \n",
      "but actually is positive.  \n",
      " \n",
      "The Credibility of the model is based on how many correct predictions the model did.  \n",
      " \n",
      "What is the accuracy of the machine learning model for this classification task?\n",
      " \n",
      "Accuracy represents the number of correctly classified data instances over the total \n",
      "number of data instances.  \n",
      "In this example, Accuracy = (55 + 30)/(55 + 5 + 30 + 10 ) = 0.85 and in percentage the \n",
      "accuracy will be 85%.  \n",
      "Is accuracy the best measure?  \n",
      "Accura cy may not be a good measure if the dataset is not balanced (both negative and \n",
      "positive classes have different numbers of data instances). We will explain this with an \n",
      "example.  \n",
      "Consider the following scenario: There are 90 people who are healthy (negative)  and \n",
      "10 people who have some disease (positive). Now let’s say our machine learning \n",
      "model perfectly classified the 90 people as healthy but it also classified the unhealthy \n",
      "people as healthy. What will happen in this scenario? Let us see the confusion matr ix \n",
      "and find out the accuracy?  \n",
      "In this example, TN = 90, FP = 0, FN = 10 and TP = 0. The confusion matrix is as \n",
      "follows.  \n",
      " \n",
      "\n",
      " \n",
      "Figure 7: Confusion matrix for healthy vs unhealthy people classification task.  \n",
      " \n",
      "Accuracy in this case will be (90 + 0)/(100) = 0.9 a nd in percentage the accuracy is 90 \n",
      "%. \n",
      "Is there anything fishy?  \n",
      "The accuracy, in this case, is 90 % but this model is very poor because all the 10 \n",
      "people who are unhealthy are classified as healthy. By this example what we are trying \n",
      "to say is that accurac y is not a good metric when the data set is unbalanced. Using \n",
      "accuracy in such scenarios can result in misleading interpretation of results.  \n",
      "So now we move further to find out another metric for classification. Again we go back \n",
      "to the pregnancy classificat ion example.  \n",
      "Now we will find the precision (positive predictive value) in classifying the data \n",
      "instances. Precision is defined as follows:  \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "What does precision mean?  \n",
      "Precision should ideally be 1 (high) for a good classifier. Precision becomes 1 only \n",
      "when the numerator and denominator are equal i.e TP = TP +FP, this also means FP \n",
      "is zero. As FP increases the value of the denominator becomes greater than the \n",
      "numerator a nd precision value decreases (which we don’t want).  \n",
      "So in the pregnancy example, precision = 30/(30+ 5) = 0.857  \n",
      "Now we will introduce another important metric called recall. Recall is also known as \n",
      "sensitivity or true positive rate and is defined as follow s: \n",
      " \n",
      " \n",
      " \n",
      "Recall should ideally be 1 (high) for a good classifier. Recall becomes 1 only when the \n",
      "numerator and denominator are equal i.e TP = TP +FN, this also means FN is zero. \n",
      "As FN increases the value of the denominator becomes greater than the numerator \n",
      "and recall value decreases (which we don’t want).  \n",
      "So in the pregnancy example let us see what the recall will be.  \n",
      "Recall = 30/(30+ 10) = 0.75  \n",
      "So ideally in a good classifier, we want both precision and recall to be one which also \n",
      "means FP and FN are zero. Therefore we need a metric that takes into account both \n",
      "precision and recall. F1 -score is a metric which takes into account both precision and  \n",
      "recall and is defined as follows:  \n",
      " \n",
      " \n",
      " \n",
      "F1 Score becomes 1 only when precision and recall are both 1. F1 score becomes \n",
      "high only when both precision and recall are high. F1 score is the harmonic mean of \n",
      "precision and recall and is a better measure than accu racy.  \n",
      "In the pregnancy example, F1 Score = 2* ( 0.857 * 0.75)/(0.857 + 0.75) = 0.799.  \n",
      " \n",
      "\n",
      "What kind of questions do you ask your client if they give you a dataset?  \n",
      "● How was the data compiled? Was it aggregated from multiple sources? ...  \n",
      "● Is the data accurat e? ...  \n",
      "● Is the data clean? ...  \n",
      "● How much data should you have? ...  \n",
      "● Remember why: what problem do you want to tackle?  \n",
      "● Dimension of the dataset  \n",
      "● Type of the attributes in the dataset  \n",
      "● For predictive analytics, target attribute  \n",
      "● Missing values in the data set  \n",
      "● How to fill missing values?  \n",
      " \n",
      "Have you ever done an F test on your dataset, if yes, give an example. If No, then \n",
      "explain F distribution?  \n",
      "F Distribution  \n",
      "F-Test (variance ratio test)  \n",
      "When we run a regression analysis, we get f value to find out the means between two \n",
      "populations. It's similar to a T statistic from a T -Test. A T -test will tell you if a single \n",
      "variable is related statistically, and an F test will tell you if a group of va riables is jointly \n",
      "significant.  \n",
      "● F-test is used to test the two independent estimations of population \n",
      "variances(S 1^2 & S 2^2). \n",
      "● F-test is used by comparing the ratio of the two variances S 1^2 & S 2^2. \n",
      "● The samples must be independent.  \n",
      "● F-test is a small sample test.  \n",
      "● F = (Larger estimate of population variance) / (Smaller estimate Of population \n",
      "variance)  \n",
      "● The variance ratio = S 1^2 & S 2^2 \n",
      "● F-test never is -ve because the upper value is greater than lower.  \n",
      "● Degree of freedom for larger populat ion[vS1]  variance is V1[vS2]  and smaller \n",
      "V2 \n",
      "● The null hypothesis of two population variance are equal, i.e., HO: S1^2 = S 2^2 \n",
      "Determining the Values of F  \n",
      "F Distribution using Python  \n",
      "#impolrt scipy, numpy and matplotlib  \n",
      "  \n",
      "x=np.linspace( -10, 10, 100)  \n",
      "dfn = 2 9 \n",
      "dfd = 18  \n",
      "mean, var, skew, kurt = scipy.stats.f.stats(dfn, dfd, moments='mvsk')  \n",
      "print('mean: {:.2f}, skewness: {:.2f}, kurtosis: {:.2f}'.format(mean, var, skew, kurt))  \n",
      "plt.plot(x, scipy.stats.f.pdf(x,dfn, dfd))  \n",
      "plt.show()  \n",
      "  \n",
      "mean: 1.12, skewness: 0.28, kur tosis: 1.81  \n",
      "  \n",
      "Note:  \n",
      "● The Student ‘t’ distribution is robust, which means that if the population is non -\n",
      "normal, the results of the t -test and confidence interval estimate are still valid \n",
      "provided that the population is not extremely non -normal.  \n",
      "● To check this requirement, draw a histogram of the data and see how bell -\n",
      "shaped the resulting figure is. If a histogram is extremely skewed (say in that \n",
      "case of an exponential distribution), that could be considered “extremely non -\n",
      "normal,” and hence, t -statistics would not be valid in this case.  \n",
      "  \n",
      "Example  \n",
      "Question: From a population of women, suppose you randomly select 7 women, and \n",
      "from the population of men, 12 men are selected.  \n",
      "Population  Population standard deviation  Sample standard \n",
      "deviation  \n",
      "Women  30 35 \n",
      "Men 50 45 \n",
      "To calculate f statistics.  \n",
      "Answer: The f statistic can be calculated from the sample standard deviations and \n",
      "population, using the following equation:                               f = [ s 12/σ12 ] / [ s 22/σ22 ] \n",
      "where Standard deviation of the sample drawn from population 1 is s 1 and s 2 in the \n",
      "denominator is the standard deviation of the sample drawn from population 2, σ 1 is the \n",
      "standard deviation of population 1, Population 2’s standard deviation is σ 2. \n",
      "As we can  see from the equation, there are two ways to compute an f statistic from \n",
      "these data. If the data of women appears in the numerator, we can compute f statistic \n",
      "as follows:  \n",
      "f = ( 552 / 202 ) / ( 452 / 502 ) \n",
      "f = (3025 / 400) / (2025 / 2500).  \n",
      "f = 1.361 / 0.81  = 1.68  \n",
      "For calculations, the numerator degrees of freedom v 1 are 7 - 1 or 6; and the degrees \n",
      "of freedom for denominator v 2 are 12 - 1 or 11.  \n",
      "On the other hand, if the men's data appears in the numerator, we can calculate the f \n",
      "statistic as follows:  \n",
      "f = ( 452 / 502 ) / ( 552 / 202 ) \n",
      "f = (2025 / 2500) / (3025 / 400)  \n",
      "f = 0.812 / 1.3610 = 0.5955  \n",
      "For this calculation, the denominator degrees of freedom v 2 is 7 - 1 or 6 and the \n",
      "numerator degrees of freedom v 1 is 12 - 1 or 11  \n",
      "When we are trying to find the cumulative probability associated with an f statistic, you \n",
      "need to know v 1 and v 2. \n",
      "Find the cumulative probability related to each of the f statistics from  the above \n",
      "example:  \n",
      "Answer: First, we need to find the degr ees of freedom for each sample. Then, \n",
      "probabilities can be found.  \n",
      "● The sample of women’s degrees of freedom is equal to n - 1 = 7 - 1 = 6.  \n",
      "● The sample of men’s degrees of freedom is equal to n - 1 = 12 - 1 = 11.  \n",
      "Therefore, when data of women appear in the numerator, then v 1 is equal to 6; and \n",
      "then v 2 is equal to 11. And, the f statistic is equal to 1.68. So, 0.78 is the cumulative \n",
      "probability.  \n",
      "When data of men appear in the numerator, then v 1 is equal to 11; and t hen v 2 is equal \n",
      "to 6. And, the f statistic is equal to 0.595. Thus the cumulative probability is 0.22.  \n",
      " \n",
      "What is AUC & ROC Curve? Explain with uses.  \n",
      "We know that the classification algorithms work on the concept of probability of \n",
      "occurrence of the possi ble outcomes. A probability value lies between 0 and 1. Zero \n",
      "means that there is no probability of occurrence and one means that the occurrence is \n",
      "certain.  \n",
      " \n",
      "But while working with real -time data, it has been observed that we seldom get a \n",
      "perfect 0 or 1 val ue. Instead of that, we get different decimal values lying between 0 \n",
      "and 1. Now the question is if we are not getting binary probability values how are we \n",
      "actually determining the class in our classification problem?  \n",
      " \n",
      "There comes the concept of Threshold. A threshold is set, any probability value below \n",
      "the threshold is a negative outcome, and anything more than the threshold is a \n",
      "favourable or the positive outcome. For Example, if the threshold is 0.5, any probability \n",
      "value below 0.5 means a negative or an unfavourable outcome and any value above \n",
      "0.5 indicates a positive or favourable outcome.  \n",
      "Now, the question is, what should be an ideal threshold?  \n",
      " \n",
      "The horizontal lines represent the various values of thresholds ranging from 0 to 1.  \n",
      "* Let’s suppo se our classification problem was to identify the obese people from the \n",
      "given data.  \n",
      "* The green markers represent obese people and the red markers represent the non -\n",
      "obese people.  \n",
      "* Our confusion matrix will depend on the value of the threshold chosen by us . \n",
      "* For Example, if 0.25 is the threshold then  \n",
      "        TP(actually obese)=3  \n",
      "        TN(Not obese)=2  \n",
      "        FP(Not obese but predicted obese)=2(the two red squares above the 0.25 line)  \n",
      "        FN(Obese but predicted as not obese )=1(Green circle below 0.25 line  )  \n",
      " \n",
      "A typical ROC curve looks like the following figure.  \n",
      "* Mathematically, it represents the various confusion matrices for various thresholds. \n",
      "Each black dot is one confusion matrix.  \n",
      "* The green dotted line represents the scenario when the true positive rate equals the \n",
      "false positive rate.  \n",
      "* As evident from the curve, as we move from the rightmost dot towards left, after a \n",
      "certain threshold, the false positive rate decreases.  \n",
      "* After s ome time, the false positive rate becomes zero.  \n",
      "* The point encircled in green is the best point as it predicts all the values correctly and \n",
      "keeps the False positive as a minimum.  \n",
      "* But that is not a rule of thumb. Based on the requirement, we need to sele ct the point \n",
      "of a threshold.  \n",
      "* The ROC curve answers our question of which threshold to choose.  \n",
      " \n",
      "### But we are confused!!  \n",
      " \n",
      "Let’s suppose that we used different classification algorithms, and different ROCs for \n",
      "the corresponding algorithms have been plotte d. \n",
      "The question is: which algorithm to choose now?  \n",
      "The answer is to calculate the area under each ROC curve.  \n",
      "#### AUC(Area Under Curve)  \n",
      "* It helps us to choose the best model amongst the models for which we have plotted \n",
      "the ROC curves  \n",
      "* The best model is the one that encompasses the maximum area under it.  \n",
      "* In the adjacent diagram, amongst the two curves, the model that resulted in the red \n",
      "one should be chosen as it clearly covers more area than the blue one  \n",
      " \n",
      "Who decided in your last project, what will be the accuracy of your model & what was \n",
      "the criterion to make the decision.   \n",
      " \n",
      "Whether i was doing a classification problem so i have chosen parameter s for \n",
      "classification model evaluation  \n",
      "For classification m odel evaluation , we have different parameter s like performance \n",
      "matrix,pr curve, roc -auc curve in performance matrix also we have different different \n",
      "evaluation parameter s like accuracy, error rate , precision, recall so based on our class \n",
      "distribu tion, we can choose any of them, first i have checked accuracy of the model \n",
      "and also i have gone through with roc -auc curve inside that i have checked auc score \n",
      "of the given model.  \n",
      "In the auc score I had the criterion 0.5 or 50% based on that i have  filtered the model \n",
      "then i have compared auc score between the models as well so whatever auc score i \n",
      "have found greater that i have model i have chosen finally.  \n",
      " \n",
      "What do you understand by 1 tail test & 2 tail test? Give an example.  \n",
      "If the alternate hy pothesis gives the alternate in both directions (less than and greater \n",
      "than) of the value of the parameter specified in the null hypothesis, it is called a Two  \n",
      "tailed test . \n",
      "If the alternate hypothesis gives the alternate in only one direction (either less than or \n",
      "greater than) of the value of the parameter specified in the null hypothesis, it is called \n",
      "One tailed test . \n",
      "e.g. if H0: mean= 100            H1: mean not equal to 100  \n",
      "Here according to H1, the mean can be greater than or less than 100. This is an \n",
      "example of Two tailed test  \n",
      "Similarly, if H0: mean>=100      then H1:  mean< 100  \n",
      "Here, the mean is less than 100, it is called One tailed test.  \n",
      " \n",
      "What do you understand by the power of a test?  \n",
      " \n",
      "The statistical power of a binary hypothesis test is t he probability that the test correctly \n",
      "rejects the null hypothesis H0 when a specific alternative hypothesis H1 is true. It is \n",
      "commonly denoted by 1 -beta , and represents the chances of a \"true positive\" \n",
      "detection conditional on the actual existence of an effect to detect. Statistical power \n",
      "ranges from 0 to 1, and as the power of a test increases,  \n",
      "the probability beta  of making a type II error by wrongly failing to reject the null \n",
      "hypothesis decreases.  \n",
      " \n",
      "How do you set the level of significance for you r dataset?  \n",
      "In normal English, \"significant\" means important, while in Statistics \"significant\" means \n",
      "probably true (not due to chance). A research finding may be true without being \n",
      "important. When statisticians say a result is \"highly significant\" they mea n it is very \n",
      "probably true. They do not (necessarily) mean it is highly important.  \n",
      " \n",
      "we are determining the significance level(alpha). This refers to the likelihood of \n",
      "rejecting the null hypothesis even when it's true. A common alpha is 0.05 or 5 per  \n",
      "cent.(We can choose 1% or 10% as well)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Have you ever used a T table in any of your project s so far? If No, then why is statistics \n",
      "important for data scientists? If yes, explain the scenario.  \n",
      " \n",
      "It is the science of conducting studies to collect, organize, summarize, analyze, and \n",
      "draw a conclusion out of data.It deals with collective informative d ata, interpreting \n",
      "those data, and drawing a conclusion from that data.It is used in many disciplines like \n",
      "marketing, business, healthcare, telecom, etc.  \n",
      " \n",
      "In any data science project, data helps us to analyze the initial level of insight.  \n",
      " \n",
      "Building models u sing popular statistical methods such as Regression, Classification, \n",
      "Time Series Analysis and Hypothesis Testing which is core of data science. Data \n",
      "Scientists run suitable experiments and interpret the results with the help of these \n",
      "statistical methods.  \n",
      " \n",
      "So we have a data and based on that data we are going to create a statistical model \n",
      "which will be able to learn from data itself above some of the statics techniques has \n",
      "been given based on this scenario you can understand statistics is important with \n",
      "respect to datascience.  \n",
      " \n",
      " \n",
      " \n",
      "Can we productionise statistical model?  \n",
      " \n",
      "What is productionize  \n",
      "It means testing and deploying an application to production such that it uses real data \n",
      "on a frequent basis to produce output for use by the business. When data scientists \n",
      "build and test models, it is often a very manual process.  \n",
      " \n",
      "Optimizing data science a cross the entire enterprise requires more than just cool tools \n",
      "for wrangling and analyzing data. Obviously, we can simply hardcode a data science \n",
      "model  or rent a pre -trained predictive model in the cloud, embed it into an application \n",
      "in-house and we are do ne.yes so we can productionise our statistical model.  \n",
      " \n",
      "How frequently do you build the model and test it?  \n",
      " \n",
      "If a model’s predictive performance has fallen due to changes in the environment, the \n",
      "solution is to retrain the model on a new training set, whi ch reflects the current reality. \n",
      "How often should you retrain your model? And how do you determine your new training \n",
      "set? The answer is that it depends . But what does it depend on?  \n",
      "Sometimes the problem setting itself will suggest when to retrain your model. For \n",
      "instance, suppose you’re working for a university admissions department and are \n",
      "tasked with building a student attrition model that predicts whether a student will return \n",
      "the following semester. This model will be used to generate predictions on the current \n",
      "cohort of students directly after midterms. Students identified as being at risk of \n",
      "churning will automatically be enrolled in tutoring or some other such intervention.  \n",
      "Let’s think about the time horizon of such a model. Since we’re generating predictions \n",
      "in batches once a semester, it doesn’t make sense to retrain the model any more often \n",
      "than this because we won’t have access to any new training data. Therefore we might \n",
      "choose to retrain our model at the start of each semester after we’ve observed which \n",
      "students from the previous semester dropped out.  \n",
      "This is an example of a periodic  retraining schedule. It’s often a good idea to start with \n",
      "this simple strategy but you’ll need to determine based on your business problem \n",
      "exactly how frequently you’ll need to retrain.  \n",
      "Quickly changing training sets might require you to train as often as daily or weekly. \n",
      "Slower varying distributions might require monthly or annual retraining.  \n",
      " \n",
      " \n",
      "What are the testing techniques that you use for model testing, name some of those?  \n",
      "Answer: - \n",
      "For making the good/Quality of  model we need to perform Model Testing some of \n",
      "them are  \n",
      "In machine Learning - \n",
      "1. Hypothesis testing  \n",
      "2. Cross validation  \n",
      "3. Regularization  \n",
      "4. Accuracy  \n",
      "5. Precision  \n",
      "6. Recall  \n",
      "7. F1 score , F2 score  \n",
      "8. Confusion matrix  \n",
      "9. R2 , adjusted R2  \n",
      " \n",
      "What do you understand by sensitivity in dataset? Give example.  \n",
      "Sensitivity: -  it  is  the  basically a measure of the proportion/percentage of actual \n",
      "positive  cases predicted   out of total Positive cases (true positive) present . Sensitivity \n",
      "is also termed as Recall.  \n",
      " \n",
      "Recall/Sensitivity=(true  positives / all  actual  positives) or (TP)/(TP+FN)*100  \n",
      " \n",
      "TP = how many positively  predicted out of all actual Posit ive \n",
      "FN=  how many  negatively predict our model but they are Positive In reality.  \n",
      " \n",
      " \n",
      "For eg in hospital 100 patient is present out of them 50 are really pregnent.  \n",
      " \n",
      "For eg ,Our model predict  \n",
      "For this TP = 45  \n",
      "FN =5  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Answer : - Sensitivity = (45/45+5)*100 = 90% is our model sensitive .  \n",
      " \n",
      "Let’s suppose you are trying to solve the classification problem; how do you decide \n",
      "which algorithm to use?  \n",
      " So for Choosing any of the classification model you need think for some impor tant \n",
      "points like,  \n",
      " \n",
      "1. What is the Size of the training data. It is usually recommended to gather a good \n",
      "amount of data to get reliable  \n",
      "2.predictions Results  \n",
      "3.Accuracy  of the output. ...  \n",
      "4.Speed or Training time.  \n",
      "5.Checking is Data is having Linearity or No n-linearity.  \n",
      "6.Number of features.  \n",
      " \n",
      "You need to try with all necessary algorithm that can full fill these points.  \n",
      " \n",
      "For eg. If you need to perform some Regression Problem s over your data  \n",
      "You need look for the dataset is linear or not if linear then you can  use Linear \n",
      "Regression over it and if not then you can use Random Forest algorithm , which is the \n",
      "Robust algo. It will learn the non linear relationship between data and similarly , you \n",
      "need to look for model which can withstand your Requirements.  \n",
      " TP FN \n",
      "FP TN Predicted  \n",
      "R\n",
      "e\n",
      "a\n",
      "l \n",
      "Can we  use Logistic regression for classification if my no. of classes are 5?  \n",
      "Answer: - Yes we can use but Logistic Regression is a simple but  very effective \n",
      "classification algorithm so it is commonly used for  many binary classification tasks. \n",
      "Logistic regressio n model takes a linear equation as input and use logistic function and \n",
      "log odds to perform a binary classification task but you want to perform Multi class \n",
      "Classification(class=5) then you can use One Vs Rest logistic Regression method \n",
      "which will Divide th e whole dataset in Two part  one for single class and another it will \n",
      "consider rest as a one class and in the same way it will do classification. For multi \n",
      "class classification , there are lots of algorithm s are the  Robust.  \n",
      " \n",
      "Let’s suppose there is a company like OLA or UBER who provides service to many \n",
      "customers, then how will they make sure that car availability in particular region and \n",
      "what kind of dataset is required?  \n",
      " \n",
      "Ans: - So first of all They  are using google maps api for getting the exact co-ordinates \n",
      "of peoples  \n",
      "And also the exact co -ordinate of  the cab so that after getting the request for booking \n",
      "cab there are finding the less displacement between the cab and people by using the \n",
      "hamming distance formula so that they can provide the quick  booking, and after \n",
      "reaching the destination of people the cab got vacant an d the same way they are \n",
      "trying to compare the distance between people and cab and based on this they are \n",
      "doing this things.  \n",
      " \n",
      "AI Solution for architecture -- Let’s suppose the re is agricultural field in diff areas in \n",
      "India, and we know soil & weather condition is different over India, So I am trying to \n",
      "build system which helps me understanding what kind of treatments I will be able to \n",
      "apply on my crops, which crop I can grow in  particular month so I can be able to \n",
      "maximize the benefit form the soil. Then what kind of algorithm you will use whether \n",
      "its ML,DL, Vision? What will be your approach and what kind of solution design you \n",
      "will provide?  \n",
      " \n",
      "Ans: - We can use Machine learning t hen  this process will perform good ,first Question \n",
      "is to Classify the of Different region of india ,based on soil  types , and after that you \n",
      "need to predict the weather for that specific regions of india and also you need to \n",
      "classify the crops for based on soil and based on time and based on weather and after \n",
      "you can use any Classification algorithm which will try to learn from the data  model \n",
      "and if you enter the input data then it will be able to classify that which crop you should \n",
      "farm.  \n",
      "  \n",
      "And if you wa nt to use deep learning , then you need to use some ANN which is very \n",
      "good capa city to learn the non -linear  relationship between the feature so that it can \n",
      "give you the best prediction  \n",
      " \n",
      "Why do we need neural network s instead of straight forward traditional computing?  \n",
      " \n",
      "Answer  \n",
      "Neural networks offer a different way to analyze data, and to recognize patterns within \n",
      "that data, than traditional computing methods. However, they are not a solution for all \n",
      "computing problems. Traditional computing methods work well for problems that ca n \n",
      "be well characterized. Balancing checkbooks, keeping ledgers, and keeping tabs of \n",
      "inventory are well defined and do not require the special characteristics of neural \n",
      "networks.  \n",
      "Traditional computers are ideal for many applications. They can process data,  track \n",
      "inventories, network results, and protect equipment. These applications do not need \n",
      "the special characteristics of neural networks.  \n",
      "Expert systems are an extension of traditional computing and are sometimes called \n",
      "the fifth generation of computing. (First generation computing used switches and wires. \n",
      "The second generation occurred because of the development of the transistor. The \n",
      "third generation involved solid -state technology, the use of integrated circuits, and \n",
      "higher level languages like COBOL, F ortran, and \"C\". End user tools, \"code \n",
      "generators,\" are known as the fourth generation.) The fifth -generation involves artificial \n",
      "intelligence.  \n",
      "CHARACTERISTICS  TRADITIONAL COMPUTING  \n",
      "(including Expert Systems)  ARTIFICIAL NEURAL  \n",
      "NETWORKS  \n",
      "Processing style  \n",
      "Functions  Sequential  \n",
      "Logically (left brained)  \n",
      "via Rules Concepts  \n",
      "Calculations  Parallel  \n",
      "Gestault (right brained)  \n",
      "via Images  \n",
      "Pictures  \n",
      "Controls  \n",
      "Learning Method  \n",
      "Applications  by rules (didactically)  \n",
      "Accounting  \n",
      "word processing  \n",
      "math inventory  \n",
      "digital communications  by example \n",
      "(Socratically)  \n",
      "Sensor processing  \n",
      "speech recognition  \n",
      "pattern recognition  \n",
      "text recognition  \n",
      " \n",
      "Table 2.6.1 Comparison of Computing Approaches.  \n",
      " \n",
      "What are the different weight initialization techniques you have used?  \n",
      " \n",
      "Answer  \n",
      "1. Zero initialization  \n",
      "2. Random initialization  \n",
      "3. He initialization  \n",
      "4. Xavier initialization  \n",
      "Can you visualize a neural network? if yes provide name of the software we can use?  \n",
      " \n",
      "Answer  \n",
      "Yes, using Netron  \n",
      " \n",
      "How will you explain the training of neural network s? \n",
      " \n",
      "Answer  \n",
      "To build a good Artificial Neural Network (ANN) you will need the following ingredients  \n",
      "Artificial Neurons  (processing node) are composed of:  \n",
      "● (many)  input  neuron(s) connection(s)  \n",
      "● a computation unit  composed of:  \n",
      "● a linear function  (ax+b)  \n",
      "● an activation function   \n",
      "● an output   \n",
      " \n",
      "All Neurons  of a given  Layer  are generating an  Output, but they don’t have the \n",
      "same  Weight  for the next  Neurons Layer. This means that if a Neuron on a layer \n",
      "observes a given pattern it might mean less for the overall picture and will be partia lly \n",
      "or completely muted. This is what we call  Weighting: a  big weight means that the Input \n",
      "is important  and of course  a small weight means that we should ignore it.  \n",
      " \n",
      "Every  Neural Connection  between  Neurons  will have  an associated Weight.  \n",
      "And this is the ma gic of  Neural Network Adaptability:  Weights  will be adjusted over the \n",
      "training to fit the  objectives  we have set (recognize that a dog is a dog and that a cat \n",
      "is a cat).  In simple terms: Training a Neural Network means finding the appropriate \n",
      "Weights of th e Neural Connections thanks to a feedback loop called Gradient \n",
      "Backward propagation.  \n",
      " \n",
      "Can you please explain difference between sigmoid & tanh function.  \n",
      "Answer  \n",
      "Sigmoid function and tanh function are two activation functions used in deep learning. \n",
      "Also, they look very similar to each other. In this article, I’d like to have a quick \n",
      "comparison.  \n",
      "Sigmoid function  \n",
      "tanh function  \n",
      " \n",
      "Differences between them  \n",
      "The difference can be seen from the picture below. Sigmoid function has a range of 0 \n",
      "to 1, while tanh function has a range of -1 to 1. “In fact, tanh function is a scaled sigmoid \n",
      "function!”  \n",
      " \n",
      "\n",
      " \n",
      "Figure : - The red one is sigmoid and the green one is the tanh  function  \n",
      " \n",
      "Explain disadvantage of using RELU function.  \n",
      "The dying ReLU Problem  \n",
      "The dying ReLU problem refers to the scenario when a large number of ReLU neurons \n",
      "only output values of 0. From the red outline below, we can see that this happens when \n",
      "the inputs are in the  negative  range.  \n",
      " \n",
      "Red outline (in the negative x range) demarcati ng the horizontal segment where ReLU \n",
      "outputs 0  \n",
      " \n",
      "While this characteristic is what gives ReLU its strengths (through network sparsity), it \n",
      "becomes a problem when a majority of the inputs to these ReLU neurons is in the \n",
      "\n",
      "negative range. The worst case scenario is when the entire network dies, meaning that \n",
      "it becomes just a constant function.  \n",
      "When most of these neurons return output zero, the gradients fail to flow during \n",
      "backpropagation and the weights do not get updated. Ultimately a large part of the \n",
      "network becomes inactive and it is unable to learn furt her. \n",
      "Because the slope of ReLU in the negative  input range is also zero, once it becomes \n",
      "dead (i.e. stuck in negative range and giving output 0), it is likely to remain \n",
      "unrecoverable.  \n",
      "However, the dying ReLU problem does not happen all the time, since the optimizer \n",
      "(e.g. stochastic gradient descent) considers multiple input values each time.  As long \n",
      "as NOT all the inputs  push ReLU to the negative segment (i.e. some inputs are in \n",
      "positive range), the neurons can get to stay active, the weights can get update d, and \n",
      "the network can continue learning.  \n",
      " \n",
      "How do you select no. of layers & no. of neurons in neural network?  \n",
      "These are Hyperparameters so the exact the number is not defined. We take \n",
      "references from different research papers.  \n",
      "Have you ever desi gned any Neural network architecture by yourself?  \n",
      "Yes \n",
      "Can you please explain SWISS Function?  \n",
      " \n",
      "The Swish activation function  \n",
      "Formally stated, the Swish activation function is…  \n",
      " \n",
      "\n",
      "Like ReLU, Swish is bounded below (meaning as  x approaches negative \n",
      "infinity,  y approaches some constant value) but unbounded above (meaning \n",
      "as x approaches positive infinity,  y approaches infinity). However, unlike ReLU, Swish \n",
      "is smooth  (it does not have sudden changes of motion or a vertex):  \n",
      " \n",
      "Additi onally, Swish is  non-monotonic , meaning that there is not always a singularly \n",
      "and continually positive (or negative) derivative throughout the entire function. \n",
      "(Restated, the Swish function has a negative derivative at certain points and a positive \n",
      "derivat ive at other points, instead of only a positive derivative at all points, like Softplus \n",
      "or Sigmoid.  \n",
      "The derivative of the Swish function is…  \n",
      " \n",
      "The first and second derivatives of Swish, plotted:  \n",
      " \n",
      "For inputs less than about 1.25, the derivative has a magnitude of less than 1.  \n",
      " \n",
      " \n",
      "\n",
      "What is learning rate in laymen way and how do you control learning rate?  \n",
      " \n",
      "Answer  \n",
      "The learning rate is  a tuning parameter in an optimization algorithm that determines \n",
      "the step size at each iteration while moving toward a minimum of a loss function  \n",
      "One of the key hyperparameters to set in order to train a neural network is the learning \n",
      "rate for gradient desc ent. As a reminder, this parameter scales the magnitude of our \n",
      "weight updates in order to minimize the network's loss function.  \n",
      "If your learning rate is set too low, training will progress very slowly as you are making \n",
      "very tiny updates to the weights in y our network. However, if your learning rate is set \n",
      "too high, it can cause undesirable divergent behavior in your loss function. I'll visualize \n",
      "these cases below - if you find these visuals hard to interpret, I'd recommend reading \n",
      "(at least) the first secti on in my post on gradient descent.  \n",
      " \n",
      " \n",
      "What is diff between batch, minibatch & stochastic gradient decent.  \n",
      " \n",
      "Answer  \n",
      "Batch Gradient Descent  \n",
      "In Batch Gradient Descent, all the training data is taken into consideration to take a \n",
      "single step.  We take the average of the gradients of all the training examples and then \n",
      "use that mean gradient to update our parameters. So that’s just one step of gradi ent \n",
      "descent in one epoch.  \n",
      "Batch Gradient Descent is great for convex or relatively smooth error manifolds. In this \n",
      "case, we move somewhat directly towards an optimum solution.  \n",
      "\n",
      "Stochastic Gradient Descent  \n",
      "In Batch Gradient Descent we were considering all th e examples for every step of \n",
      "Gradient Descent. But what if our dataset is very huge. Deep learning models crave \n",
      "for data. The more the data the more chances of a model to be good. Suppose our \n",
      "dataset has 5 million examples, then just to take one step the m odel will have to \n",
      "calculate the gradients of all the 5 million examples. This does not seem an efficient \n",
      "way. To tackle this problem we have Stochastic Gradient Descent. In Stochastic \n",
      "Gradient Descent (SGD), we consider just one example at a time to take a  single step.  \n",
      "SGD can be used for larger datasets. It converges faster when the dataset is large as \n",
      "it causes updates to the parameters more frequently.  \n",
      "Mini Batch  \n",
      "Neither we use all the dataset all at once nor we use the single example at a time. We \n",
      "use a batch of a fixed number of training examples which is less than the actual dataset \n",
      "and call it a mini -batch. Doing this helps us achieve the advantages of both the former \n",
      "variants we saw.  \n",
      "So, when we are using the mini -batch gradient descent we are updat ing our \n",
      "parameters frequently as well as we can use vectorized implementation for faster \n",
      "computations.  \n",
      " \n",
      "What do you understand by batch size while training Neural N/w with example  \n",
      " \n",
      "Answer  \n",
      "The batch size is a hyperparameter that defines the number of samples to work \n",
      "through before updating the internal model parameters.  \n",
      "Think of a batch as a for -loop iterating over one or more samples and making \n",
      "predictions. At the end of the batch, the pred ictions are compared to the expected \n",
      "output variables and an error is calculated. From this error, the update algorithm is \n",
      "used to improve the model, e.g. move down along the error gradient.  \n",
      "A training dataset can be divided into one or more batches.  \n",
      "When all training samples are used to create one batch, the learning algorithm is called \n",
      "batch gradient descent. When the batch is the size of one sample, the learning \n",
      "algorithm is called stochastic gradient descent. When the batch size is more than one \n",
      "sample and less than the size of the training dataset, the learning algorithm is called \n",
      "mini-batch gradient descent.  \n",
      "● Batch Gradient Descent. Batch Size = Size of Training Set  \n",
      "● Stochastic Gradient Descent. Batch Size = 1  \n",
      "● Mini-Batch Gradient Descent. 1 < Batch Size < Size of Training Set  \n",
      "In the case of mini -batch gradient descent, popular batch sizes include 32, 64, and 128 \n",
      "samples.  \n",
      "Explain 5 best optimizer you know with mathematical explanation.  \n",
      "Answer  \n",
      "Stochastic Grad ient Descent  \n",
      "It’s a variant of Gradient Descent. It tries to update the model’s parameters more \n",
      "frequently. In this, the model parameters are altered after computation of loss on each \n",
      "training example. So, if the dataset contains 1000 rows SGD will update the model \n",
      "parameters 1000 times in one cycle of dataset instead of one time as in Gradient \n",
      "Descent.  \n",
      "θ=θ−α⋅∇J(θ;x(i);y(i)) , where {x(i) ,y(i)} are the training examples . \n",
      "As the model parameters are frequently updated parameters have high variance and \n",
      "fluctuations in loss functions at different intensities.  \n",
      "Advantages : \n",
      "1. Frequent updates of model parameters hence, converges in less time.  \n",
      "2. Requires less memory as no need to store values of loss functions.  \n",
      "3. May get new minima’s.  \n",
      "Disadvantages : \n",
      "1. High variance in model parameters.  \n",
      "2. May shoot even after achieving global minima.  \n",
      "3. To get the same convergence as gradient descent needs to slowly reduce the \n",
      "value of learning rate.  \n",
      "Mini-Batch Gradient Descent  \n",
      "It’s best among all the variations of gradient descent algorithms. It is an improvement \n",
      "on both SGD and standard gradient descent. It updates the model parameters after \n",
      "every batch. So, the dataset is  divided into various batches and after every batch, the \n",
      "parameters are updated.  \n",
      "θ=θ−α⋅∇J(θ; B(i)), where {B(i)} are the batches of training examples . \n",
      "Advantages : \n",
      "1. Frequently updates the model parameters and also has less variance.  \n",
      "2. Requires medium amount of memory.  \n",
      "All types of Gradient Descent have some challenges:  \n",
      "1. Choosing an optimum value of the learning rate. If the learning rate is too small \n",
      "than gradient descent may take ages to converge.  \n",
      "2. Have a constant learning rate for all t he parameters. There may be some \n",
      "parameters which we may not want to change at the same rate.  \n",
      "3. May get trapped at local minima.  \n",
      " \n",
      "Adagrad  \n",
      "One of the disadvantages of all the optimizers explained is that the learning rate is \n",
      "constant for all parameters and fo r each cycle. This optimizer changes the learning \n",
      "rate. It changes the learning rate  ‘η’ for each parameter and at every time step  ‘t’. It’s \n",
      "a type second order optimization algorithm. It works on the derivative of an error \n",
      "function.  \n",
      " \n",
      "A derivative of loss function for given parameters at a given time t.  \n",
      " \n",
      "Update parameters for given input i and at time/iteration t  \n",
      "η is a learning rate which is modified for given parameter  θ(i) at a given time based on \n",
      "previous gradients calculated for g iven parameter  θ(i). \n",
      "We store the sum of the squares of the gradients w.r.t.  θ(i) up to time step  t, while  ϵ is \n",
      "a smoothing term that avoids division by zero (usually on the order of 1e−8). \n",
      "Interestingly, without the square root operation, the algorithm pe rforms much worse.  \n",
      "\n",
      "It makes big updates for less frequent parameters and a small step for frequent \n",
      "parameters.  \n",
      "Advantages : \n",
      "1. Learning rate changes for each training parameter.  \n",
      "2. Don’t need to manually tune the learning rate.  \n",
      "3. Able to train on sparse data.  \n",
      "Disadvantages : \n",
      "1. Computationally expensive as a need to calculate the second order derivative.  \n",
      "2. The learning rate is always decreasing results in slow training.  \n",
      "AdaDelta  \n",
      "It is an extension of  AdaGrad  which tends to remove the  decaying learning \n",
      "Rate  problem of it. Instead of accumulating all previously squared \n",
      "gradients,  Adadelta  limits the window of accumulated past gradients to some fixed \n",
      "size w. In this exponentially moving average is used rather than the sum of all the \n",
      "gradients.  \n",
      "E[g²](t)=γ.E[g²](t−1)+(1−γ).g²(t)  \n",
      "We set  γ to a similar value as the momentum term, around 0.9.  \n",
      " \n",
      "Update the parameters  \n",
      "Advantages : \n",
      "1. Now the learning rate does not decay and the training does not stop.  \n",
      "Disadvantages : \n",
      "\n",
      "1. Computationally expensive.  \n",
      " \n",
      "Adam  \n",
      "Adam  (Adaptive Moment Estimation) works with momentums of first and second order. \n",
      "The intuition behind the Adam is that we don’t want to roll so fast just because we can \n",
      "jump over the minimum, we want to  decrease the velocity a little bit for a careful search. \n",
      "In addition to storing an exponentially decaying average of past squared gradients \n",
      "like AdaDelta , Adam  also keeps an exponentially decaying average of past \n",
      "gradients  M(t).  \n",
      "M(t) and V(t)  are values of the first moment which is the  Mean  and the second moment \n",
      "which is the  uncentered variance  of the gradients  respectively.  \n",
      " \n",
      "First and second order of momentum  \n",
      "Here, we are taking mean of  M(t) and V(t) so that  E[m(t)]  can be equal \n",
      "to E[g(t)]  where,  E[f(x)]  is an expected value of  f(x). \n",
      "To update the parameter:  \n",
      " \n",
      "Update the parameters  \n",
      "The values for β1 is 0.9 , 0.999 for β2, and (10 x exp( -8)) for ‘ ϵ’. \n",
      "Advantages : \n",
      "1. The method is too fast and converges rapidly.  \n",
      "2. Rectifies vanishing learning rate, high variance.  \n",
      "Disadvantages : \n",
      "\n",
      "Computationally costly.  \n",
      " \n",
      " \n",
      "Can you build Neural network without using any library? If yes, prove it.  \n",
      " \n",
      "Answer  \n",
      "Notebook Link: - https://colab.research.google.com/drive/1iNt4eKU6PjDG_ygv -\n",
      "_FhcuBhumeI -v3B \n",
      "What is use of biases in neural network?  \n",
      "Answer  \n",
      "The activation function in Neural Netw orks takes an input 'x' multiplied by a weight 'w'. \n",
      "Bias allows you to shift the activation function by adding a constant (i.e. the given bias) \n",
      "to the input. Bias in Neural Networks can be thought of as analogous to the role of a \n",
      "constant in a linear funct ion, whereby the line is effectively transposed by the constant \n",
      "value.  \n",
      " \n",
      "In a scenario with no bias, the input to the activation function is 'x' multiplied by the \n",
      "connection weight 'w 0'. \n",
      " \n",
      "\n",
      "In a scenario with bias, the input to the activation function is 'x' times the connection \n",
      "weight 'w 0' plus the bias times the connection weight for the bias 'w 1'. This has the \n",
      "effect of shifting the activation function by a constant amount (b * w 1). \n",
      " \n",
      "How do you do hyper -parameter tuning for neural network  \n",
      " \n",
      "Answer  \n",
      "1. Step 1 — Deciding on the network topology  \n",
      "2. Step 2 — Adjusting the learning rate. ...  \n",
      "3. Step 3 — Choosing an optimizer and a loss function. ...  \n",
      "4. Step 4 — Deciding on the batch size and number of e pochs. ...  \n",
      "5. Step 5 — Random restarts.  \n",
      "6. Step 6 – Define the Input Shape  \n",
      "7. Step 7 - Choose the Right activation function  \n",
      "8. Step 8 – Choosing the no of kernels and layers in CNN  \n",
      " \n",
      "What kind of regularization you used wrt neural network.  \n",
      "L2 Parameter Regularization  \n",
      "This regularization is popularly known as weight decay. This strategy drives the \n",
      "weights closer to the origin by adding the regularization term omega which is defined \n",
      "as: \n",
      " \n",
      "This technique is also known as ridge regression or Tikhonov regularization.  \n",
      " \n",
      "L1 Regularization  \n",
      "Here the regularization term is defined as:\n",
      " \n",
      "\n",
      "Dataset Augmentation  \n",
      "The best and easiest way to make a model generalize is to train it on a large amount \n",
      "of data b ut mostly we are provided with limited data. One way is to create fake data \n",
      "and add it to our training dataset, for some domains this is fairly straightforward and \n",
      "easy.  \n",
      "Noise Robustness  \n",
      "Noise is often introduced to the inputs as a dataset augmentation str ategy. the addition \n",
      "of noise with infinitesimal variance at the input of the model is equivalent to imposing \n",
      "a penalty on the norm of the weights. Noise injection is much more powerful than \n",
      "simply shrinking the parameters, especially when the noise is adde d to the hidden \n",
      "units.  \n",
      "Early Stopping of Training  \n",
      "When training a large model on a sufficiently large dataset, if the training is done for a \n",
      "long amount of time rather than increasing the generalization capability of the model, \n",
      "it increases the overfitting. As in the training process, the training error k eeps on \n",
      "reducing but after a certain point, the validation error starts to increase hence signifying \n",
      "that our model has started to overfit.  \n",
      "Dropout  \n",
      "Dropout is a computationally inexpensive but powerful regularization method, dropout \n",
      "can be thought of as a method of making bagging practical for ensembles of very many \n",
      "large neural networks. The method of bagging cannot be directly applied to large \n",
      "neural networks as it involves training multiple models, and evaluating multiple models \n",
      "on each test example. sin ce training and evaluating such networks is costly in terms \n",
      "of runtime and memory, this method is impractical for neural networks.  \n",
      "Bagging  \n",
      "Bagging or bootstrap aggregating is a technique for reducing generalization error by \n",
      "combining several models. The id ea is to train several different models separately, \n",
      "then have all of the models vote on the output for test examples. This is an example of \n",
      "a general strategy in machine learning called model averaging. Techniques employing \n",
      "this strategy are knownas ensemb le methods. This is an efficient method as different \n",
      "models don’t make the same types of errors.  \n",
      " \n",
      " \n",
      "What are the libraries you have used for neural network implementation?  \n",
      " \n",
      "Answer  \n",
      "Keras Tuner, Optuna, HyperOPT, Tune  \n",
      "What do you understand by custom layer and a custom model?  \n",
      " \n",
      "Answer  \n",
      "Reference Link : - \n",
      "https://keras.io/guides/making_new_layers_and_models_via_subclassing  \n",
      "How do you implement differentiation using TensorFlow or Pytorch library?  \n",
      " \n",
      "Answer  \n",
      "Tensorflow : - https://jonathan -hui.medium.com/tensorflow -automatic -differentiation -\n",
      "autodiff -1a70763285cb  \n",
      "Notebook Link : - \n",
      "https://colab.research.g oogle.com/github/tensorflow/tensorflow/blob/r1.9/tensorflow/c\n",
      "ontrib/eager/python/examples/notebooks/automatic_differentiation.ipynb  \n",
      "Pytorch  \n",
      "Using  autograd  to Find and Solve a Derivative  \n",
      " \n",
      "First, it should be obvious that we have to represent our original fu nction in Python as \n",
      "such:  \n",
      "y = 5*x**4 + 3*x**3 + 7*x**2 + 9*x - 5 \n",
      " \n",
      "import  torch  \n",
      " \n",
      "x = torch .autograd .Variable(torch .Tensor([ 2]),requires_grad =True ) \n",
      "y = 5*x**4 + 3*x**3 + 7*x**2 + 9*x - 5 \n",
      " \n",
      "y.backward()  \n",
      "x.grad  \n",
      " \n",
      "Line by line, the above code:  \n",
      "● imports the torch library  \n",
      "● defines the function we want to compute the derivative of  \n",
      "● defines the value (2) we want to compute the derivative with regard to as a \n",
      "PyTorch  Variable  object and specifies that it should be instantiated in such a way \n",
      "that it track s where in the computation graph it connects to in order to perform \n",
      "differentiation by the chain rule (requires_grad)  \n",
      "● uses  autograd's  backward()  to compute the su m of gradients, using the chain rule  \n",
      "● outputs the value stored in the  x tensor's  grad attribute, which, as shown below  \n",
      "tensor([ 233.])  \n",
      " \n",
      "This value, 233, matches what we calculated by hand, above.  \n",
      " \n",
      "What is meaning of epoch in simple terms?  \n",
      " \n",
      "Answer  \n",
      "An epoch is  a term used in machine learning  and indicates the number of passes of \n",
      "the entire training dataset the machine learning algorithm has completed. Datasets are \n",
      "usually grouped into batches (especially when the amount of data is very large).  \n",
      "The number of epochs is a hyperparameter that defines the number times that the \n",
      "learning algorithm will work through the entire training dataset.  \n",
      "One epoch means that each sample in the training dataset has had an opportunity to \n",
      "update the internal model p arameters. An epoch is comprised of one or more batches. \n",
      "For example, as above, an epoch that has one batch is called the batch gradient \n",
      "descent learning algorithm.  \n",
      "You can think of a for -loop over the number of epochs where each loop proceeds over \n",
      "the tra ining dataset. Within this for -loop is another nested for -loop that iterates over \n",
      "each batch of samples, where one batch has the specified “batch size” number of \n",
      "samples.  \n",
      " \n",
      "What do you understand by a TensorFlow record?  \n",
      " \n",
      "Answer  \n",
      "The TFRecord format is  a simple format for storing a sequence of binary records . \n",
      "Protocol buffers are a cross -platform, cross -language library for efficient serialization \n",
      "of structured data. Protocol messages are defined by . proto files, these are often the \n",
      "easiest way to under stand a message type.  \n",
      "More Depth : - https://www.tensorflow.org/tutorials/load_data/tfrecord  \n",
      "Explain the technique for doing data augmentation in deep learning  \n",
      " \n",
      "Answer  \n",
      "Data augmentation  is the technique of increasing the size of data used for training a \n",
      "model. For reliable predictions, the deep learning models often require a lot of training \n",
      "data, which is not always available. Therefore, the existing data is augmented in order \n",
      "to make a better generalized model.  \n",
      "Although data augmentation can be applied in various domains, it's commonly used in \n",
      "computer vision. Some of the most common data augmentation techniques used for \n",
      "images are:  \n",
      "● Position augmentation  \n",
      "● Scaling  \n",
      "● Cropping  \n",
      "● Flipping  \n",
      "● Padding  \n",
      "● Rotation  \n",
      "● Translation  \n",
      "● Affine transformation  \n",
      "● Color augmentation  \n",
      "● Brightness  \n",
      "● Contrast  \n",
      "● Saturation  \n",
      "● Hue \n",
      " \n",
      " \n",
      "List down diff CNN network you heard of.  \n",
      "1. LeNet  \n",
      "2. AlexNet  \n",
      "3. ZFNet  \n",
      "4. Inception (GoogLeNet)  \n",
      "5. VGG  \n",
      "6. ResNet (MSRA)  \n",
      "7. ResNeXt  \n",
      "8. SENet  \n",
      "9. PNASNet  \n",
      "10. EfficientNet  \n",
      "11. DenseNet  \n",
      " \n",
      "List down a names of object detection algorithm you know  \n",
      "1. RCNN  \n",
      "2. Fasster RCNN  \n",
      "3. Faster RCnn  \n",
      "4. Yolo  \n",
      "5. SSD  \n",
      "6. CenterNet  \n",
      "What is difference between object detection and classification?  \n",
      "Consider the below image:  \n",
      " \n",
      "You will have instantly recognized it. It’s a dog.  \n",
      "There’s only one object here: a dog. We can easily use image classification model and \n",
      "predict that there’s a dog in the given image. But what if we have both a cat and a dog \n",
      "in a single image?  \n",
      " \n",
      "We can tr ain a multi -label classifier, in that instance. Now, there’s another caveat - we \n",
      "won’t know the location of either animal/object in the image.  \n",
      "Image Localization   helps us to identify the location of a single object in the given \n",
      "image. In case we have mult iple objects present, we then rely on the concept of  Object \n",
      "Detection . We can predict the location along with the class for each object using OD.  \n",
      " \n",
      " \n",
      "List down major tasks we perform in CNN.  \n",
      "1. Image Classification  \n",
      "2. Object Detection  \n",
      "\n",
      "3. Image Segmentaion  \n",
      "4. Image Captioning  \n",
      "5. Visual Question Answering  \n",
      "6. Image Generat ion \n",
      "List down algorithms for segmentation  \n",
      "1. Region -Based Segmentation  \n",
      "1. Threshold Segmentation  \n",
      "2. Regional Growth Segmentation  \n",
      "2. Edge Detection Segmentation  \n",
      "1. Sobel Operator  \n",
      "2. Laplacian Operator  \n",
      "3. Segmentation based on Clustering  \n",
      "Reference: - https://arxiv.org/ftp/arxiv/papers/1707/1707.02051.pdf  \n",
      " \n",
      "What was kind of evaluation you were doing in the production environment?  \n",
      "What you should consider more often in a production scenario is  revenue  for your \n",
      "model, and A/B test is a must  Besides  , you can check if the distribution of your \n",
      "prediction is consistent with that of ground truth concerning accuracy and stability for \n",
      "your model . \n",
      "What was no. of requests (hits) your model was receiving on daily basis?  \n",
      "40K requests on daily basis. We h ave checked the log in the past and found that we \n",
      "usually receive 2.5k to 3.5k requests per hour for the model. Job is scheduled to \n",
      "process the bulk of requests from a file. The file size is usually about 370 MB to 440MB. \n",
      "Job is scheduled for the fixed tim e at night.  \n",
      "How you have implemented logging in the project for any failure cases?  \n",
      "We have implemented logging in our system. It captures each request and its status \n",
      "whether it was successful or failed. Even the exception of the system is captured \n",
      "prop erly. None of the detail can be missed.  \n",
      "We have the policy to keep a log of the previous 6 months and the remaining log we \n",
      "archive into backup tables.  \n",
      "We only keep a 13 -month log including an archival log.  \n",
      "How you have integrated a notification (or Ala rm) system for your project?  \n",
      "We have implemented our custom notification program. It sends notification emails to \n",
      "our team.  \n",
      "We have an application where an incident gets created automatically if any job failed \n",
      "or any exception occurs in our system. We have  an SLA of 99.9%. Our application has \n",
      "been operational it’s repose time has to be less than 1.2 seconds. Else we get a \n",
      "notification then we investigate.  \n",
      "How you have implemented model monitoring?  \n",
      "Few of model monitoring we have implemented:  \n",
      "Model Inpu t Monitoring:  the set of expected values for an in put feature, we can check \n",
      "that  \n",
      "a) the input values fall within an allowed set (for categorical inputs) or range (for \n",
      "numerical inputs) and  \n",
      "b) that the frequencies of each respective value within the set a lign with what we have \n",
      "seen in the past  \n",
      "Depending on our model configuration, we will allow certain input features to be null \n",
      "or not. This is something we can monitor. If features we expect generally to  not be the \n",
      "null start to change, that could indicate a data skew or change in consumer behavior, \n",
      "both of which would be cause for further investigation.  \n",
      "Model Response time: The number of requests that can be processed data validation \n",
      "and cleaning was also included.  \n",
      "Model Versioning: Usually we retrain our m odel every weekend concerning new data \n",
      "we receive.  \n",
      "How you have derived the final KPI (Key Performance Indicator) for your client?  \n",
      "How many dashboards were there in your project?  \n",
      "We have 2 dashboards one is concerning application and one is for our  model \n",
      "performance.  \n",
      "We only use it to work model performance -related dashboard.  \n",
      "Successful train rate. In our system, we do out -of-core learning because of the huge \n",
      "dataset. Our team is working to transform training steps in the production environment. \n",
      "We have started adopting mops currently we have not fully transformed our application \n",
      "with MLOps practices within the next 6 months our team will do it. We have started the \n",
      "planning.  \n",
      "On which platform do you have productions your model?  \n",
      "We have deployed o ur model at the AKS cluster. Sometimes we need to scale our API \n",
      "capabilities so we are using AKS.  \n",
      "What kind of API you have exposed to receive data for the model?.  \n",
      "We have a user interface to upload a file. Where user uploads their file in the backend \n",
      "file will be uploaded to s3 bucket. And we did the configuration in such a way that it \n",
      "can pick the file from s3 buck and it will start the validation file. If the file will be validated \n",
      "successfully. We transform data for prediction and again we upload th e file at the s3 \n",
      "bucket. We have a UI where users can see their prediction file generate for the \n",
      "uploaded file. Users will get a notification that the file is available you can download \n",
      "the file.  \n",
      "What was the size of your final production environment ( system configuration)?  \n",
      "We have 64 GB RAM.  \n",
      "We have GPU and CPU in our system. We have 5 TB of Hard disk.  \n",
      "What and all Databases you have used in the project?  \n",
      "The application uses a Microsoft SQL Server. But logging related to our machine \n",
      "learning operat ion we used MongoDB.  \n",
      "What kind of optimization you have done in your project, to what depth & explain the \n",
      "example.  \n",
      " \n",
      "Can you please talk about complete team structure and team size?  \n",
      "Our team was divided into  MLOps, developers, data engineers, data scientists. 24 \n",
      "members were there in my team.  \n",
      "What was the duration of your complete project?  \n",
      "My project was all about 9 months.  \n",
      "What was your day -to-day responsibility in the last 2 months?  \n",
      "We usually try various random experiments and trying to a nalyze changes in data and \n",
      "estimate whether we should incorporate another mechanism to transform our data to \n",
      "get a more generalized prediction. Our meeting is scheduled with the client on weekly \n",
      "basis. If we want to highlight anything.  \n",
      "Previous it was dail y as we were working proactively client decided to switch on weekly \n",
      "basis.  \n",
      "What kind of change request you have been receiving after your productions project  \n",
      "What kind of testing you have done in development, UAT, pre -pod, and prod?  \n",
      "Stress testing.  Integration testing. Of course model evaluation is the core part of \n",
      "testing.  \n",
      "We tested API.  We also simulated malicious requests to check robustness.  \n",
      "Have you used some of the predefined AI -OPS pipelines if yes explain.  \n",
      "We implement AIOps not complet ed but you can 60%. We have done the data version \n",
      "by ourself. We develop logic for that. We have deployed our application AKS.  \n",
      "We did Data Version COntro, CI and CD.  \n",
      "CML was not entirely implementing. We have to manually trigger training if it’s required \n",
      "or Schedule the training at a certain time.  \n",
      "Who has implemented AI -OPS in your project?  \n",
      "AIOPS team has done. 2 person was in AIOPS team who designed the whole \n",
      "workflow. We have used github and dvc and mlflow and circle ci actions.  \n",
      "What was the OPS stack you have been using?  \n",
      "GITHUB, GIT, DVC MLFLOW, CIRCLE CI , DOCKER HUB, elastic container service  \n",
      "What do you understand by CI -CD & have you implemented those in your project? If \n",
      "yes, what was the tech  \n",
      "stack you used for the CI -CD pipeline?  \n",
      "CI: Frequent changes have to be integrated with the whole application on daily basis. \n",
      "We should have automated test cases to check against every checkin of code. a \n",
      "notification has to be sent if test cases pass or fail.  \n",
      "CD: Our application can be deployed  at any anytime so if we want to release a new \n",
      "version we should not discuss what date would be best because with help of a CD we \n",
      "can do it on daily basis but you should be careful if it’s required to meet demand  \n",
      "GITHUB, GIT, CIRCLECI,  DOCKERHUB, and HERO KU. \n",
      "What was the biggest challenge you faced in the project and how you have resolved \n",
      "it? \n",
      "Data was huge we can not train our model on whole data at once we impleted \n",
      "incremental learning.  \n",
      "Where we have created mini -batches of the dataset and then traine d our model.  \n",
      "Give me one scenario where you worked as a team player?  \n",
      "What was your overall learning from the current project?  \n",
      "I have seen end -to-end real work machine learning application. I have seen how team \n",
      "work is important to success for proje ct delivery timeline.  One of the most important \n",
      "like project planning and defining milestones to achieve within specific days and \n",
      "ensuring overtime if everything is going as per the plan if not what is the reason. What \n",
      "is another alternative to resolve th e problem for time being. Evaluating risk and \n",
      "develop a strategy to encounter the worst -case scenarios.  \n",
      "Technically: I got familiar with cloud infrastructure and  MLOps practices. Different \n",
      "types of orchestration toll -like airflow  \n",
      "How do you keep your self updated on new technology?  \n",
      "I devote few hours to learn new technology on daily basis. I don’t push myself for long \n",
      "hour continuous learning. I usually decide what tech I am interested in then I use to \n",
      "start exploring those.  \n",
      "Have you designed an ar chitecture for this project? If yes, define a strategy wrt to your \n",
      "current project.  \n",
      "No, I haven’t designed architecture for the project.  \n",
      "How many images you have taken to train your DL model?  \n",
      "80K \n",
      "What is the size of the model that you have in your product ion system?  \n",
      "It was around some KB. But I am not sure what was the exact size of my model. It may \n",
      "be aound 150 KB to 250 KB  \n",
      "Have you tried optimizing this Vision or DL model?  \n",
      "Where you have hosted your Computer Vison model?  \n",
      "What was your frame per secon d? \n",
      "What is the data filtration strategy you have defined for the CV project in production?  \n",
      "Have you used any edge device in this project, if yes, why?  \n",
      "What was the name of the camera & camera quality?  \n",
      "What was the outcome you were generating from these dev ices?  \n",
      "Have you processed the data in the local system or the cloud? Give reason.  \n",
      "How many number devices do you have productions (camera, edge devices, etc.)  \n",
      "Let’s suppose I am trying to build a solution to count the no. of the vehicle or to detect \n",
      "their no. plate or track their  \n",
      "speed. Then what is the dependency of distance, position & angle of the camera on \n",
      "your final model? What will  \n",
      "happen to your model? if we change position angle.  \n",
      "What was your data collection strategy in the CV project, have you received data from \n",
      "the client or you have  \n",
      "created the data? And how you have implemented it?  \n",
      "What is difference between Euclidian distance and Manhattan distance. Explain in \n",
      "simple words.  \n",
      " \n",
      "Manhattan Distance: It is also called the Taxicab distance or the City Block distance. \n",
      "It is the distance between two points measured along axes at right angles.  \n",
      " \n",
      "● ManhattanDistance = sum for i to N sum |x[i] – y[i]| \n",
      " \n",
      " \n",
      "Euclidean Distance: It is the straight line distance between two  data points in a plane. \n",
      "It is calculated using Minkowski Distance formula by setting ‘p’ value to 2, also known \n",
      "as the ‘L2’ norm distance metric.  \n",
      " \n",
      "● EuclideanDistance = sqrt(sum for i to N (x[i] – y[i])^2)  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What do you understand by feature selection, t ransformation, engineering and EDA & \n",
      "What are the steps  \n",
      "that you have performed in each of these in detail with example.  \n",
      " \n",
      " \n",
      "Feature Selection: Selection of features with the highest influence on the target \n",
      "variable, from a set of existing features.  \n",
      "This can  be done with various techniques: Linear Regression, Decision Trees, \n",
      "calculation of \"importance\" weights  \n",
      " \n",
      "Feature Transformation:  Transformation of features in order to create new ones based \n",
      "on the old ones to improve the accuracy of the algorithm  \n",
      "A popul ar technique is Principal Component Analysis  \n",
      " \n",
      "Feature Engineering: Generation of features which is in a format that is difficult to \n",
      "analyse directly and are not directly comparable. Ex: images, time -series, etc.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What is difference between single values decomposition (SVD) and PCA?  (hint: SVD \n",
      "is one of the way to do  \n",
      "PCA)  \n",
      " \n",
      " \n",
      " \n",
      "PCA: Principal Component Analysis (PCA) is a statistical procedure that uses an \n",
      "orthogonal transformation that converts a set of correlated var iables to a set of \n",
      "uncorrelated variables. PCA is the most widely used tool in exploratory data analysis \n",
      "and in machine learning for predictive models.  \n",
      "SVD: The Singular -Value Decomposition, is a matrix decomposition method for \n",
      "reducing a matrix to its con stituent parts in order to make certain subsequent matrix \n",
      "calculations simpler.  \n",
      " \n",
      "A = U . Sigma . V^T  \n",
      " \n",
      "Where A is the real m x n matrix that we wish to decompose, U is an m x m matrix, \n",
      "Sigma (often represented by the uppercase Greek letter Sigma) is an m x n diagonal \n",
      "matrix, and V^T is the  transpose of an n x n matrix where T is a superscript.  \n",
      " \n",
      " \n",
      "What ki nd of feature transformations have you done in your last project?  \n",
      " \n",
      "Feature transformation is the process of modifying data but keeping the information.  \n",
      "Few thing we have done is:  \n",
      " \n",
      "Data Smoothing  \n",
      "Data Aggregation  \n",
      "Generalization  \n",
      "Normalization  \n",
      " \n",
      " \n",
      "Have you taken any external features in any of the projects from any 3rd party data? \n",
      "If yes, explain that scenario.  \n",
      " \n",
      " \n",
      "Yes, while predicting the covid cases from different states, we had to measure the \n",
      "oxygen production capacity of that state. If not then t he nearest oxygen producer state. \n",
      "For this purpose , we had used 3rd party data.  \n",
      "If your model is overfitted, what will you do next?  \n",
      "Overfitting of the model means Low bias with High variance  \n",
      "There are the number  of techniques available to handle overfitting : \n",
      "Cross -validation: This is done by splitting your dataset into ‘test’ data and ‘train’ data. \n",
      "Build the model using the ‘train’ set. The ‘test’ set is used for in -time validation.  \n",
      "Regularization: It regularizes or shrinks the coefficient estimates towards  zero.  \n",
      "Early stopping: This prevents the model from memorizing the dataset.  \n",
      "Pruning: This technique applies to decision trees.  \n",
      "Pre-pruning: Stop ‘growing’ the tree earlier before it perfectly classifies the \n",
      "training set.  \n",
      "Post -pruning: This a llows the tree to ‘g row’, perfectly classify the training set \n",
      "and then post prune the tree.  \n",
      "Dropout: This is a technique where randomly selected neurons are ignored during \n",
      "training.  \n",
      " \n",
      "Explain me the bias-variance trade -off. \n",
      " \n",
      " \n",
      "If our model is too simple and has very few parameters , then it may have high bias and \n",
      "low variance. But, o n the other hand , if our model has a large number of parameters, \n",
      "it will  have high variance and low bias. So we need to find an excellent  balance withou t \n",
      "overfitting and underfitting the data.  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What steps would you take to improve accuracy of your model? At -least mention 5 \n",
      "approach. And justify  \n",
      "why would you choose those approach  \n",
      "Few steps to improve the accuracy of a model:  \n",
      "Add more data: Adding more data will help to learn the model in a good way instead \n",
      "of relying on assumption s and weak correlations.  \n",
      " \n",
      "Handle missing and outlier values: The p resence of missing and outlier values often \n",
      "reduces the accuracy of a model or leads to  a biased model  because we don’t analyse \n",
      "the relationship with another variable correctly. So it is essential  to handle missing and \n",
      "outlier value.  \n",
      " \n",
      "Feature Engineering: It will help to extract more information from existing data. It may \n",
      "have high ability to  explain the variance in the training data.  \n",
      " \n",
      "Feature Selection: It will help to find the best attribute which better explains the \n",
      "relationship of independent variable with target variable.  \n",
      " \n",
      "\n",
      "Multiple algorithm s: Some algorithm is better suited to a particul ar type of dataset than \n",
      "other. Hence we should apply all relevant model s and check the performance.  \n",
      " \n",
      "Algorithm Tuning: The objective of parameter tuning is to find the optimum value for \n",
      "each parameter to improve the model's accuracy . \n",
      " \n",
      "Explain process of feature engineering in context of text categorization.  \n",
      " \n",
      " \n",
      " Language detection: Understand which natural \n",
      "language data is in.  \n",
      " Text preprocessing: Preparing raw data to make it \n",
      "suitable for machine learning model. Ex: text \n",
      "cleaning, stopword  removal, stemming and \n",
      "lemmatization  \n",
      " Length analysis: It’s important to have a look at the \n",
      "length of the text because it’s an easy calculation that \n",
      "can give a lot of insights.  \n",
      " Sentiment analysis: determine whether a text is \n",
      "positive or negative.  \n",
      " Named -Entity recognition: It is a process to tag text \n",
      "with pre -defined categories such as person names, \n",
      "organizations, locations.  \n",
      " Word frequency: find the importance of single word \n",
      "by computing the n -gram frequency.  \n",
      " Word vectors: transform a word into numbers.  \n",
      " Topic  modeling: extract the main topics from corpus.  \n",
      " \n",
      " \n",
      " \n",
      "Explain vectorization and hamming distance.  \n",
      " \n",
      "Vectorization: It is a technique by which we can make our code execute very fast. It \n",
      "takes multiple iterative operations among data points and turn them into ma trix \n",
      "operation. Matrix operations are fast, they can be parallelize to some extent  \n",
      " \n",
      "Hamming distance: Hamming distance is a metric for comparing two binary data \n",
      "strings. While comparing two binary strings of equal length, Hamming distance is the \n",
      "number of bit positions in which the two bits are different.  \n",
      "The Hamming distance between two strings, a and b is denoted as d(a,b).  \n",
      " \n",
      "Can you please explain chain rule and its use?  \n",
      " \n",
      "Suppose cost is calculated as follows, the input is x and the target value is y, \n",
      "f’= f(x)  \n",
      "g’=g(x)  \n",
      "y’= k(g’)  \n",
      "cost= criterion(y,y’)  \n",
      " \n",
      "If you want to calculate d(cost) / d(x), x  can be a number, a vector, or a matrix. You \n",
      "can calculate d(f’) / d(x)  ⨉ d(g’) / d(f’)  ⨉ d(cost) / y’ to get d(cost) / d(x). In machine \n",
      "learning, the three functions here, f, g, k  represent different mappings, and the criterion \n",
      "is also understood as a mapping, except that the input here adds the target value y. \n",
      "The x here represents the input data, but the meaning of the input v alue is not \n",
      "significant  because we can’t change the data to make our target cost smaller. The \n",
      "actual situation is to change the variable contained in each map. The variables are \n",
      "derived. For example, if you use w𝚏 to represent the variable in function f, you can now \n",
      "calculate the derivative of cost to w𝚏. You can calculate it as follows. Before displaying \n",
      "the calculation method, rewrite the previous expression here. Include w𝚏, \n",
      "f’= f(x, w𝚏 ) \n",
      "g’= g(f’)  \n",
      "y’= k(g’)  \n",
      "cost= criterion(y,y’)  \n",
      "d(cost)/d( w𝚏 )= d{f’)/d(w𝚏 ) * d(g ’)/d(g’) * d(y’)/d(g’) * d(cost)/y ’ \n",
      "This is from the chain rule of calculus.  \n",
      " \n",
      " \n",
      "What is the difference between correlation and covariance?  \n",
      " \n",
      "Covariance  Correlation  \n",
      "Covariance is a measure to indicate the \n",
      "extent to which two random variables \n",
      "change in tandem.  Correlation is a measure used to represent \n",
      "how strongly two random variables are \n",
      "related to each other.  \n",
      "Covariance is nothing but a measure of \n",
      "correlation.  Correlation refers to the scaled form of \n",
      "covariance.  \n",
      "Covariance indicates the direction of the \n",
      "linear relationship between variables.  Correlation on the other hand measures \n",
      "both the strength and direction of the linear \n",
      "relationship between two variables.  \n",
      "Covariance can vary between -∞ and +∞  Correlation ranges between -1 and +1  \n",
      "Covariance is affected by the change in \n",
      "scale.  Correlation is not influenced by the \n",
      "change in scale.  \n",
      "Covariance assumes the units from the \n",
      "product of the units of the two variab les. Correlation is dimensionless, i.e. It’s a \n",
      "unit-free measure of the relationship \n",
      "between variables.  \n",
      "Covariance of two dependent variables \n",
      "measures how much in real quantity (i.e. \n",
      "cm, kg, liters) on average they co -vary.  The c orrelation of two dependent \n",
      "variables measures the proportion of how \n",
      "much on average , these variables vary \n",
      "w.r.t one another.  \n",
      "Covariance is zero in case of independent \n",
      "variables  Independent movements do not contribute \n",
      "to the total correlation.  \n",
      " \n",
      "What are the sampling techniques you have used in your project?  \n",
      " \n",
      " \n",
      "Probability Sampling:  \n",
      " Simple random sampling  \n",
      " Stratified sampling  \n",
      " Systematic sampling  \n",
      " Cluster sampling  \n",
      "Multi stage sampling  \n",
      "Non-Probability Sampling:  \n",
      " Convenience Sampling  \n",
      " Purposive Sam pling  \n",
      " Quota sampling  \n",
      " Referral/Snowball sampling  \n",
      " \n",
      " \n",
      "Have you ever used Hypothesis testing in your last project, if yes, explain How?  \n",
      " \n",
      "Hypothesis testing is a statistical method that is used in making statistical decision \n",
      "using experimental data. It is basi cally an assumption that we make about population \n",
      "parameter.  \n",
      " \n",
      "Ex: In a heart disease prediction project we took the hypothesis that Depression \n",
      "increases the risk for coronary heart disease in established diabetes.  \n",
      " \n",
      " \n",
      "In which case you will use naïve Bayes c lassifier and decision tree separately?  \n",
      "What is the adv & disadvantage of naïve Bayes classifier, explain  \n",
      "In case of numerical data what is naïve Bayes classification equation you will use?  \n",
      "Naive Bayes is used a lot in robotics and computer vision, and does quite well with \n",
      "those tasks. Decision trees perform very poorly in those situations.  \n",
      "Decision trees are neat because they tell you what inputs are the best predicators of \n",
      "the outputs . Decision trees can often guide you to find  a statistical relationship between \n",
      "a given input to the output and how strong that relationship is.  \n",
      " \n",
      "Advantages  \n",
      "● This algorithm works quickly and can save a lot of time.  \n",
      "● Naive Bayes is suitable for solv ing multi -class prediction problems.  \n",
      "● If its assumption of the independence of features holds true, it can perform \n",
      "better than other models and requires much less training data.  \n",
      "● Naive Bayes is better suited for categorical input variables than numerical \n",
      "variables.  \n",
      "Disadvantages  \n",
      "● Naive Bayes assumes that all predictors (or features) are independent, rarely \n",
      "happening in real life. This limits the applicability of this algorithm in real -world \n",
      "use cases.  \n",
      "● This algorithm faces the ‘zero -frequency problem’ where i t assigns zero \n",
      "probability to a categorical variable whose category in the test data set wasn’t \n",
      "available in the training dataset. It would be best if you used a smoothing \n",
      "technique to overcome this issue.  \n",
      "● Its estimations can be wrong in some cases, so you  shouldn’t take its prob ability \n",
      "outputs very seriously.  \n",
      "Give me scenario where I will be able to use a boosting classifier and regressor?  \n",
      "Boosting can be used for regression as well as for classification problems. However, \n",
      "mainly focusing  on reducing bias, the base models  often considered for boosting are \n",
      "models with low variance but high bias.  \n",
      "Regression analysis is used to predict a continuous dependent variable from several  \n",
      "independent variables.  \n",
      " \n",
      "In case of Bayesian classifier what exactly it tries to learn. Define its learning \n",
      "procedure.  \n",
      "Working of Naïve Bayes' Classifier can be understood with the help of the  below \n",
      "example:  \n",
      "Suppose we have a dataset of weather conditions  and corresponding target variable \n",
      "\"Play\". So using this dataset we need to decide whether we should play or not on a \n",
      "particular day according to the weather conditions. So to solve this problem, we need \n",
      "to follow the below steps:  \n",
      "1. Convert the given dataset into frequency tables.  \n",
      "2. Generate Likelihood table by finding the probabilities of given features.  \n",
      "3. Now, use Bayes theorem to calculate the posterior probability.  \n",
      " \n",
      "Bayes theorem provides a way of calculating posterior probability P(c|x) from P(c), P(x) \n",
      "and P(x|c).  \n",
      " \n",
      "P(c|x)= P(x|c)P( c)/P(x)  \n",
      " \n",
      "Above,  \n",
      "● P(c|x) is the posterior probability of class  (c, target ) given predictor  (x, \n",
      "attributes ). \n",
      "● P(c) is the prior probability of class . \n",
      "● P(x|c) is the likelihood which is the probability of the predictor given class . \n",
      "● P(x) is th e prior probability of the predictor.  \n",
      " \n",
      "Give me a situation where I will be able to use SVM instead of Logistic regression.  \n",
      " \n",
      "If the use case is we have to find if the student has passed or not,  \n",
      "or in a supermarket , if a customer will purchase a product or not,  \n",
      "in such cases we can use Logistic regression over SVM.  \n",
      " \n",
      "What do you understand by rbf kernel in SVM?  \n",
      "RBF kernels are the most generalized form of kernelization and is one of the most \n",
      "widely used kernels due to its similarity to the Gaussian distribution. The RBF kernel \n",
      "function for two points X₁ and X₂ computes the similarity or how close they are to each  \n",
      "other. This kernel can be mathematically represented as follows:  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "Where,  \n",
      "‘σ’ is the variance and our hyperparameter  \n",
      "||X₁ - X₂|| is the Euclidean (L ₂-norm) Distance between two points X₁ and X₂  \n",
      " \n",
      " \n",
      " \n",
      "Give me 2 scenarios where AI can be used to increase revenue of travel industry.  \n",
      " \n",
      "\n",
      "1. Smarter traffic light algorithms & real -time tracking can control higher and lower \n",
      "traffic patterns effectively, This can be applied to public transport for optimal \n",
      "schedulin g & routing.  \n",
      " \n",
      " \n",
      "2. Autonomous Rail Rapid Transit is a train system that runs without rails, \n",
      "operating instead on a virtual painted track which the train’s computer system \n",
      "detects and follows  \n",
      " \n",
      "What do you understand by leaf node in decision tree?  \n",
      "A decision  tree is a structure that includes a root node, branches, and leaf nodes. Each \n",
      "internal node denotes a test on an attribute, each branch denotes the outcome of a \n",
      "test, and each leaf node holds a class label.  \n",
      "What is information gain & Entropy in decision tree?  \n",
      "Entropy is the measures of impurity, disorder or uncertainty in a bunch of examples.  \n",
      "Entropy= -∑p(X)logp(x)  \n",
      "Information gain (IG) measures how much “information” a feature gives us about the \n",
      "class.  \n",
      " \n",
      "IG= entropy(parent) - [weighted average] * entropy(children)  \n",
      "Give disadvantages of using Decision tree  \n",
      " \n",
      "1. A small change in the data can cause a large change in the decision tree \n",
      "structure,  causing instability.  \n",
      "2. For a Decision tree sometimes calcu lation can go far more complex \n",
      "compared to other algorithms.  \n",
      "3. Decision tree often involves higher time to train the model.  \n",
      "4. Decision tree training is relatively expensive as the complexity and time has \n",
      "taken are more.  \n",
      "5. The Decision Tree algorithm is inadequat e for applying regression and \n",
      "predicting continuous values.  \n",
      "List some of the features of random forest.  \n",
      "● It reduces overfitting in decision trees and helps to improve the accuracy.  \n",
      "● It is flexible to both classification and regression problems.  \n",
      "● It works well with both categorical and continuous values.  \n",
      "● It automates missing values present in the data.  \n",
      " \n",
      "How can you avoid overfitting in decision tree?  \n",
      " \n",
      "Pruning: Pruning refers to a technique to remove the parts of the decision tree to \n",
      "prevent gr owing to its full depth. By tuning the hyperparameters of the decision tree \n",
      "model one can prune the trees and prevent them from overfitting.  \n",
      " \n",
      "Pre-Pruning: The pre -pruning technique refers to the early stopping of the growth of \n",
      "the decision tree.  \n",
      " \n",
      "Post -Pruning: The Post -pruning technique allows the decision tree model to grow to \n",
      "its full depth, then removes the tree branches to prevent the model from overfitting.  \n",
      " \n",
      "Explain polynomial regression in your own way.  \n",
      " \n",
      "Polynomial Regression is a regression algorithm that models the relationship between \n",
      "a dependent(y) and independent variable(x) as nth degree polynomial.  \n",
      " \n",
      "y= b 0+b1x1+ b 2x12+ b 2x13+...... b nx1n \n",
      " \n",
      "It is also called the special case of Multiple Linear Regressi on in ML. Because we add \n",
      "some polynomial terms to the Multiple Linear regression equation to convert it into \n",
      "Polynomial Regression.  \n",
      "Explain learning mechanism of linear regression.  \n",
      " \n",
      "● Regression is a supervised machine learning technique which is used to predict \n",
      "continuous values.  \n",
      "● The ultimate goal of the regression algorithm is to plot a best -fit line or a curve \n",
      "between the data.  \n",
      "● The three main metrics that are used for evaluating the trained regression \n",
      "model are variance, bias and error. If the variance is high, it leads to overfitting \n",
      "and when the bias is high, it leads to underfitting.  \n",
      "● Based on the number of input features and output labels, regression is \n",
      "classified as linear (one input and one output), multiple (many inputs and one \n",
      "output) and multivar iate (many outputs).  \n",
      "● Linear regression allows us to plot a linear equation, i.e., a straight line. We \n",
      "need to tune the coefficient and bias of the linear equation over the training data \n",
      "for accurate predictions.  \n",
      "● The tuning of coefficient and bias is achiev ed through gradient descent or a cost \n",
      "function — least squares method.  \n",
      "What is the cost function in logistic regression?  \n",
      " \n",
      " \n",
      "The cost function in Logistic Regression is Log Loss:  \n",
      "  \n",
      "Log Loss is the most important classification metric based on probabilities. It’s hard to \n",
      "interpret raw log -loss values, but log -loss is still a good metric for comparing models. \n",
      "For any given problem, a lower log loss value means better predictions.  \n",
      " \n",
      "Log Loss = ∑ ( x , y ) ∈ D − y log ⁡ ( y ′ ) − ( 1 − y ) log ⁡(1 -y’) \n",
      " \n",
      "What is the error function in linear regression?  \n",
      " \n",
      " \n",
      "Mean squared error (MSE) is the most commonly used loss function for regression. \n",
      "The loss is the mean overseen data of the squared differences between true and \n",
      "predicted values  \n",
      " \n",
      "MSE is calculated by: measuring the distance of the  observed y -values from the  \n",
      "predicted y -values at each value of x; squaring each of these distances; calculating \n",
      "the mean of each of the squared distances.  \n",
      " \n",
      "What is the use of implementing OLS technique wrt dataset?  \n",
      "In statistics, ordinary least squares  (OLS ) is a type of linear least squares method for \n",
      "estimating the unknown parameters in a linear regression model. OLS chooses the \n",
      "parameters of a linear function of a set of explanatory variables by the principle of least \n",
      "squares: minimizing the sum of the sq uares of the differences between the observed \n",
      "dependent variable in the given dataset  \n",
      " \n",
      "Explain the dendrogram in your own way.  \n",
      "A dendrogram is a diagram that shows the hierarchical relationship between objects. \n",
      "It is most commonly created as an output from hierarchical clustering. The main use \n",
      "of a dendrogram is to work out the best way to allocate objects to clusters.  \n",
      " \n",
      "How do you measure quality of clusters in DBSCAN?  \n",
      "There are two methods to measure quality of clusters in DBSCAN which are Silhouette \n",
      "Method and Visual Cluster Interpretation.  \n",
      "Silhouette Method: This method determines the separability of clusters. To begin, an \n",
      "average distance between each point and all other points in a cluster is calculated. \n",
      "The distance between each point and each point in other clusters is then calculated. \n",
      "We divide by whichever average is bigger after subtracting the two averages.  \n",
      "In the end, we desire a high (i.e., near to 1) score, which indicates a short intra -cluster \n",
      "average distance (tight clusters) and a large inter -cluster average distance (clusters \n",
      "well separated).  \n",
      "Visual Cluster Interpretation:  It's important to understand  each cluster when you've \n",
      "gotten your clusters. This is usually accomplished by merging the original dataset with \n",
      "the clusters and viewing each cluster separately. The more prominent and obvious \n",
      "each cluster is, the better.  \n",
      " \n",
      "How do you evaluate DBSCAN a lgorithm?  \n",
      "We can evaluate DBSAN algorithm with Mean Silhouette Coefficient.  \n",
      "The Silhouette Coefficient has bounded a range of 1 to -1. 1 is the best value, and -1 \n",
      "is the worst. A higher score suggests that the model's clusters are more defined and \n",
      "denser. Negative values typically indicate that data points have been assigned to the \n",
      "wrong clusters. Values close to 0 imply overlapping clusters, whereas negative values \n",
      "usually indicate that data points have been assigned to the wrong clusters.  \n",
      "The silhouette c oefficient is calculated using two scores:  \n",
      " a: The average distance between one data point and the rest of the data points \n",
      "in the same cluster.  \n",
      " b: The average distance between one data point and the next closest cluster's \n",
      "other points.  \n",
      "                                                   \n",
      " \n",
      "What do you understand by market basket analysis?  \n",
      "Market basket analysis is a data mining approach used by merchants to better \n",
      "understand customer purchase patterns and thereby enhance revenue. It entails \n",
      "evaluating hu ge data sets, such as purchase histories, to identify product groups and \n",
      "products that are likely to be bought together.  \n",
      "The introduction of electronic point -of-sale (POS) systems boosted the implementation \n",
      "of market basket analysis. The digital records ge nerated by POS systems made it \n",
      "easier for apps to process and analyse massive volumes of purchase data when \n",
      "compared to handwritten records held by store owners.  \n",
      " \n",
      "Explain centroid formation technique in K Means algorithm.  \n",
      "A centroid is an imaginary or r eal location that represents the cluster's centre. By \n",
      "lowering the in -cluster sum of squares, each data point is assigned to one of the \n",
      "clusters.  \n",
      "To put it another way, the K -means algorithm finds k centroids and then assigns each \n",
      "data point to the closest  cluster while keeping the centroids as small as possible.The \n",
      "average of the data, or determining the centroid, is what the ‘means' in K -means refers \n",
      "to. \n",
      "Have you ever used SVM regression in any of your project, If yes, why?  \n",
      "Yes, I had used in one of my  projects because Support Vector Regression (SVR) \n",
      "recognises the existence of non -linearity in the data and offers a reliable prediction \n",
      "model.  \n",
      " \n",
      "Explain the concept of GINI Impurity.  \n",
      "\n",
      "We need to understand Entropy first for better understanding of Gini Impurity.  \n",
      "Entropy:  Entropy helps us to build an appropriate decision tree for selecting the best \n",
      "splitter. The entropy of a sub split can be defined as a measure of its purity. Entropy is \n",
      "always between 0 and 1. This formula can be used to compute the entr opy of any split.  \n",
      "                                                                                    \n",
      " \n",
      " \n",
      "Gini Impurity: Gini Impurity is a measure of the likelihood of a new instance of a random \n",
      "variable being incorrectly categorised if it were randomly cl assified using the \n",
      "distribution of class labels from the data set.  \n",
      "If the data set comprises only one class, the Gini impurity is lower bounded by 0.  \n",
      "             \n",
      " \n",
      "Let’s suppose I have given you dataset with 100 columns  how you will be able to \n",
      "control growth of decision tree?  \n",
      "First, I'll apply a dimensionality reduction on the dataset by using PCA, Intractive \n",
      "binning (IB). Methods. Then applying Decision tree on it and check via tree chart.  \n",
      "In the decision tree chart, ea ch internal node has a decision rule that splits the data. \n",
      "Gini referred to as the Gini ratio, which measures the impurity of the node. You can \n",
      "say a node is pure when all of its records belong to the same class, such nodes known \n",
      "as the leaf node.  \n",
      " \n",
      "If you are using Ada -boost algorithm & if it is giving you underfitted result What is the \n",
      "hyperparameter tuning you will do?  \n",
      "\n",
      "Will use different parameters in hyperparameter tuning which are base_estimator, \n",
      "estimators, learning_rate and random_state. The variab les we supply to a model \n",
      "before we start the modelling process are known as hyper -parameters. Let's have a \n",
      "look at them all.  \n",
      " base estimator:  The ensemble's model; by default, a decision tree is used.  \n",
      " n estimators:  The number of models that will be create d. \n",
      " learning rate: reduces each classifier's contribution by this amount.  \n",
      " random state:  The seed for the random number generator, which ensures that the \n",
      "same random numbers are created each time.  \n",
      "Will do some twitching in these parameters and getting the good result.  \n",
      "Explain gradient boosting algorithm.  \n",
      "Gradient boosting is a sort of boosting used in machine learning. It is based on the \n",
      "assumption that when the best potential next model is coupled with prior models, the \n",
      "overall prediction error is minim ised. To decrease error, the fundamental notion is to \n",
      "specify the target outcomes for the next model. How are the goals determined? The \n",
      "goal outcome for each case in the data is determined by how much modifying the \n",
      "prediction for that case affects the over all prediction error:  \n",
      " If a slight change in a case's prediction results in a big reduction in error, the \n",
      "case's next target outcome is a high value. The error will be reduced if the new \n",
      "model's predictions are near to its targets.  \n",
      " If a slight modification in a case's prediction generates no change in error, the \n",
      "case's next target result is zero. Changes to this prediction have no effect on \n",
      "the error.  \n",
      "Can we use PCA to reduce dimensionality of highly non -linear data.  \n",
      "No, PCA cannot handle non -linear data . \n",
      " \n",
      "How do you evaluate performance of PCA.  \n",
      "Reconstruction error is one way to measure performance. Indeed, one way to think of \n",
      "PCA is that it reduces the amount of data on the training set. To project the points into \n",
      "a low -dimensional space, use PCA. T hen, by projecting the low -dimensional \n",
      "representations back into the original, high -dimensional space, recreate the original \n",
      "points. The distance between the original points and their reconstructions is inversely \n",
      "proportional to the model's ability to capt ure the data's structure. This is related to \n",
      "PCA's reputation as a lossy data compression method. The original points can be \n",
      "recreated more precisely when the low -dimensional representation contains more \n",
      "information. The commonly used performance measure R 2 can also be calculated \n",
      "using reconstruction error (fraction of variance accounted for).  \n",
      " \n",
      "Have you ever used multiple dimensionality techniques in any project? if yes, give \n",
      "reason. If no, where can we use it?  \n",
      "In One of my projects was to estimate churn using the enormous data set. The \n",
      "enormous dimensionality of this data set, with 15K data columns, is its unique feature. \n",
      "Most data mining techniques are implemented column -by-column, which makes them \n",
      "slower and slower as the number of data columns gr ows. The project's first milestone \n",
      "was to lower the number of columns in the data collection while sacrificing the least \n",
      "amount of information possible. So, I used PCA (Principal Component analysis) for \n",
      "dimensionality reduction.  \n",
      " \n",
      "What do you understand  by curse of dimensionality explain.  \n",
      "When working with high -dimensional data, the \"Curse of Dimensionality\" refers to a \n",
      "set of issues. The number of attributes/features in a dataset corresponds to the \n",
      "dataset's dimension. High dimensional data is a dataset  containing a large number of \n",
      "attributes, usually on the order of a hundred or more. Some of the challenges that \n",
      "come with high -dimensional data show up while analysing or displaying the data to \n",
      "look for trends, and others show up when training machine lea rning models. The \n",
      "‘Curse of Dimensionality' refers to the difficulty in training machine learning models \n",
      "due to high dimensional data. ‘Data sparsity' and ‘distance concentration' are two well -\n",
      "known characteristics of the curse of dimensionality.  \n",
      " \n",
      "What  is the difference between anomaly detection and novelty detection?  \n",
      "Anomaly detection (also known as outlier analysis) is a data mining step that detects \n",
      "data points, events, and/or observations that differ from the expected behaviour of a \n",
      "dataset. Atypica l data might reveal significant situations, such as a technical fault, or \n",
      "prospective possibilities, such as a shift in consumer behaviour. Anomaly detection is \n",
      "increasingly automated thanks to machine learning.  \n",
      "Novelty detection, as the name implies, is t he process of identifying new or uncommon \n",
      "data inside a dataset. Outliers, also known as anomalies, are frequently detected as a \n",
      "result of their variances from the rest of the data. However, novelty detection \n",
      "algorithms may need to be tweaked to look for g roups or bursts of uncommon data \n",
      "rather than single incidences of odd data. Cluster analysis is an approach that is \n",
      "commonly used in bank fraud algorithms to monitor suspicious activity patterns.  \n",
      " \n",
      "Explain gaussian mixture model.  \n",
      "A Gaussian mixture mode l (GMM) is a type of probabilistic model in which all data \n",
      "points are generated from a mixture of finite Gaussian distributions with unknown \n",
      "parameters. The parameters for Gaussian mixture models are produced from a well -\n",
      "trained prior model using either ma ximum a posteriori estimation or an iterative \n",
      "expectation -maximization approach. When it comes to modelling data, especially data \n",
      "from multiple groups, Gaussian mixture models are quite effective.  \n",
      " \n",
      "Explain neural network in terms of mathematical function.  \n",
      "Two principles govern the operation of a neural network.  \n",
      " Forward Propagation  \n",
      " Backward Propagation  \n",
      "  \n",
      "Let's use an example to better comprehend these building blocks. To make the \n",
      "understanding obvious, I am considering a single input layer, hidden layer, and output \n",
      "layer.  \n",
      "Forward Propagation  \n",
      "                              \n",
      " \n",
      "1. Given that we have data, we'd like to use binary classification to obtain the \n",
      "desired result.  \n",
      "2. Consider a sample with features such as X1 and X2, which will be used to \n",
      "forecast the  outcome using a series of processes.  \n",
      "3. Each feature is assigned a weight, with X1, X2 representing features and W1, \n",
      "W2 representing weights. These are fed into a neuron as input.  \n",
      "4. Both functions are carried out by a neuron. a) Activation b) Summation  \n",
      "5. All fea tures are multiplied by their weights in the summing, and bias is totalled. \n",
      "(Y=W1X1+W2X2+b).  \n",
      "6. This summing function is used in conjunction with an Activation function. The \n",
      "output of this neuron is multiplied by the weight W3 and fed to the output layer \n",
      "as input.  \n",
      "7. Each neuron goes through the same procedure, although the activation \n",
      "functions in hidden layer neurons differ from those in the output layer.  \n",
      "We just initialised the weights at random and carried on with the process. Initializing \n",
      "the weights can be done in a variety of ways. But you might be wondering how these \n",
      "weights are updated, right??? Back propagation will be used to answer this question.  \n",
      "\n",
      "Backward Propagation  \n",
      "Let us return to our calculus basics, and we will update the weights using the chain \n",
      "rule that we learnt in school.  \n",
      "Chain Rule  \n",
      "The chain rule is a method for computing the derivative of composite functions, with \n",
      "the number of functions in the composition influencing the number of differentiation \n",
      "steps required. If a composite function f(x) is defined as, for example,  \n",
      "                                                            \n",
      " \n",
      "Let's take a single neuron and apply the chain rule to it.  \n",
      "                               \n",
      " \n",
      "Our primary goal in neural networks will be to reduce error, which will ne cessitate \n",
      "updating all weights via backpropagation. We need to determine a change in weights \n",
      "that will result in the least amount of inaccuracy. To do so, we use the dE/dW1 and \n",
      "dE/dW2 formulas.  \n",
      "\n",
      "                                 \n",
      " \n",
      "                                     \n",
      " \n",
      "After you've calculated the changes in weights concern mistake, we'll use the gradient \n",
      "descent process to update the weights.  \n",
      "                                     \n",
      " \n",
      "\n",
      "For all samples, forward and backward propagation wi ll continue until the error hits a \n",
      "minimum value.  \n",
      " \n",
      "Can you please corelate a biological neuron and artificial neuron?  \n",
      "The complexity of biological neural networks far exceeds that of DNNs, making \n",
      "understanding the representations they learn even more d ifficult. As a result, both \n",
      "machine learning and computational neuroscience face a similar problem: how can we \n",
      "evaluate their representations to learn how they handle complex problems? We look \n",
      "at how computational neuroscientists' data -analysis concepts an d methodologies may \n",
      "be applied to analysing representations in DNNs, and how recently discovered DNN -\n",
      "analysis approaches can be applied to understanding representations in biological \n",
      "neural networks.  \n",
      "By figure you can analyse the difference:  \n",
      " \n",
      "                          \n",
      "\n",
      "                             \n",
      "Give list of cost functions you heard of.   \n",
      "    List of cost functions used in Neural Networks.  \n",
      "A cost function is a quantitative assessment of a model's fit quality: how well it \n",
      "reproduces the data. A cost f unction is a single number that represents the sum of the \n",
      "model's divergence from the true value for all points in the dataset.  \n",
      "1. Quadratic Cost function: regression  \n",
      "         \n",
      "2. Cross Entropy Cost: Classification  \n",
      "          \n",
      "3. Exponential Cost  \n",
      " \n",
      "\n",
      "4. Hellinger Distance  \n",
      " \n",
      "5. Kullback -Leibler Divergence  \n",
      " \n",
      "6. Generalized Kullback -Leibler Divergence  \n",
      " \n",
      "7.  Itakura -Saito Distance  \n",
      " \n",
      "Can I solve problem of classification with tabular data in neural network?  \n",
      "Yes, OfCourse  We can solve tabular data classification problem in neural network.  \n",
      " \n",
      "What do you understand by backword propagation in neural network?  \n",
      "The central technique by which artificial neural networks learn is backpropagation. It \n",
      "is doing the messenger's job to inform the neural network whether it made a mistake \n",
      "when making a forecast.  \n",
      "To propagate something (light, sound, motion, or information) means to send it in a \n",
      "certain direction or through a specific medium. Backpropagate is to communicate \n",
      "anything in r esponse, or to send information back upstream - in this case, to fix an \n",
      "\n",
      "error. When we talk about backpropagation in deep learning, we're talking about \n",
      "information transfer, and that information is related to the error that the neural network \n",
      "produces when  it makes a guess about data. Correction is synonymous with \n",
      "backpropagation.  \n",
      " \n",
      "List down, time series algorithms that you know?  \n",
      " Autoregression (AR)  \n",
      " Moving Average (MA)  \n",
      " Autoregressive Moving Average (ARMA)  \n",
      " Autoregressive Integrated Moving Average (ARIMA)  \n",
      " Seasonal Autoregressive Integrated Moving -Average (SARIMA)  \n",
      " Seasonal Autoregressive Integrated Moving -Average with Exogenous \n",
      "Regressors (SARIMAX)  \n",
      " \n",
      "How to solve TS problems using deep learning?  \n",
      "LSTM – Long Term Short Memory algorithm which is a variant of RNN which is a deep \n",
      "neural network is widely us ed in TS problems because of its capability to remember \n",
      "information for a long time. By this if there is any seasonality or pattern in the data it \n",
      "can remember it and give some importance of it.  \n",
      " \n",
      "Give application of TS in weather, financial, healthca re & network analysis?  \n",
      "Weather – Weather forecasting  \n",
      "Financial – Stock Price Prediction  \n",
      "Healthcare – Covid Cases Forecasting  \n",
      "Network Analysis – Predicting the computer traffic  \n",
      "What is diff between uptrend and downtrend in TS?  \n",
      "The criteria are :  \n",
      "The o bservations must be independent of each other,  \n",
      "The number of observations must be fixed,  \n",
      "The probability of success is same for each outcome  \n",
      "A real time example is Lottery ticket where there is only two ways either you reach \n",
      "success or failure  \n",
      " \n",
      "What do you understand by seasonality in  TS? \n",
      "Seasonality refers to the  presence of some variations in trends occurring at specific \n",
      "regular intervals of time in a nutshell like a seasonal pattern.  \n",
      "For example the price of mango is cheap at summer season  \n",
      " \n",
      "What do you understand by Cyclic pattern in your TS data?  \n",
      "A cyclic pattern is similar to seasonality it shows some kind of patterns in the data but \n",
      "not in a fixed period. On an average the data exhibits this kind of a pattern at a \n",
      "minimum of two years  \n",
      "  \n",
      " \n",
      " \n",
      " \n",
      "How will you find Trend in TS Data?  \n",
      " A specific time window is selected  \n",
      " In that window see how the data is  \n",
      " For example take a span of 3 months for the sales of a company. Here in 1st the \n",
      "sales seems to be increasing but not a peak and in the 2nd month the sales is \n",
      "at a peak  and 3rd month there is a drop here in the first month there we can see \n",
      "an uptrend and in the 3rd month we can see a downtrend  \n",
      " \n",
      "Have you implemented ARCH model in TS? If yes, give scenario?  \n",
      "Yes I have implemented ARCH model for a  time series data in one of my projects \n",
      "where I had a lot of variance in the data up and down.  \n",
      "What is VAR (vector autoregressive) model?  \n",
      "Vector Auto Regressive model is a time series forecasting algorithm that can be used \n",
      "when we have two or more ti me series data which will influence each other. So here \n",
      "we can say that the relationship is bidirectional  \n",
      " \n",
      " \n",
      " \n",
      "What do you understand by univariant and multivariant TS Analysis?   \n",
      "Univariant Analysis:  \n",
      " As the name suggests univariant  analysis means analysing the time series data \n",
      "which has only feature to analyse  \n",
      "For eg: Analysing the stock’s close price  \n",
      "Multivariant Analysis:  \n",
      " As the name suggests multivariant analysis means analysing the time series \n",
      "data which has more than one feat ure to analyse straight opposite to the univariant \n",
      "analysis  \n",
      "For eg: Analysing the stock’s close price along with the volume  \n",
      " \n",
      "Give example where you have created a multivariant model?  \n",
      "I created a Vector AutoRegressive model which is a multivariate mode l in one of my \n",
      "project where I was supposed to use the Air Quality Index data and forecast the value \n",
      "for each particulate some of the particulates are CO, CO2, NO2, etc..  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "What do you understand by p, d, & q in ARIMA model ? \n",
      "p(AR)  – the number of au toregressive terms  \n",
      "d(I) – the number of non -seasonal differences  needed for stationarity  \n",
      "q(MA)  – the number of lagged forecast errors  \n",
      "These are some characteristics of ARIMA(p, d, q) model  \n",
      "Tell me mechanism by which I can find p, d, q in ARIMA model?  \n",
      "p – We should make a partial autocorrelation plot and see till where there is an \n",
      "exponential decrease which is the shutoff point that value is taken for p  \n",
      "q – A similar kind of process like p is used to calculate q but the difference is here we \n",
      "will be us ing autocorrelation plot  \n",
      "d – It is found by seeing the seasonal difference shift  \n",
      "What is SARIMA and how it’s different from ARIMA?   \n",
      "SARIMA which is known as seasonal ARIMA model and is an extension of ARIMA \n",
      "model. The difference between ARIMA and SARIMA models is the seasonality of the \n",
      "dataset. If the data is not seasonal we can use ARIMA or if the data is seasonal we \n",
      "can use SARIMA  \n",
      " \n",
      " \n",
      "What is meaning of AR, MA and I in ARIMA model?  \n",
      "AR – Auto Regressive  \n",
      "MA – Moving Average  \n",
      "I – Integration  \n",
      "ARIMA model is a model which combines both AR and MA models along with a \n",
      "differencing preprocessing step of sequence in purpose to make it stationary called \n",
      "Integration  \n",
      " \n",
      "Can we solve TS problems with transformers? W hat is your thought on that? why do \n",
      "you think in that way?  \n",
      "Yes we can solve Time Series problem using Transformers. But it should be the last \n",
      "resort at any point of time because Transformers is a heavy model so the output \n",
      "generating time at real time will be very slow and even for training t he model is \n",
      "computationally intensive compared to the conventional Time Series models like \n",
      "ARIMA, SARIMAX or else NN’s like LSTM or RNN. But if we are you going to use it \n",
      "we should start from just a single attention layer with a considerable learning rate  \n",
      " \n",
      "Have you ever productionised TS Based Model using LSTM? What are adv antages  \n",
      "and disadvantages ? \n",
      " \n",
      "Can we solve TS problem using Regressive algorithm, if yes, why, if no, give a reason?  \n",
      "Yes we can solve Time Series problem using regressive algorithms like XGBoost, \n",
      "Linear Regression, etc.. but this wont work for a realtime time series data because our \n",
      "model needs to understand the patterns in the data and capture it and give some \n",
      "importanc e it that’s where the power Time Series models comes into picture. So \n",
      "though we can convert TS problem into a regressive problem it is not advisable to do  \n",
      " \n",
      "Explain ACF and PACF plots  \n",
      "A correlogram (also called Auto Correlation Function ACF Plot or Autocorrelation plot) \n",
      "is a visual way to show serial correlation in data that changes over time  (i.e. time series \n",
      "data). Serial correlation (also called autocorrelation) is where an error at one point in \n",
      "time travels to a subsequent point in time.  \n",
      "The PACF plot  is a plot of the partial correlation coefficients between the series and \n",
      "lags of itself.  \n",
      "Arim a and Sarima, which to use when?  \n",
      "ARIMA  is an acronym for “autoregressive integrated moving average.” It's a model \n",
      "used in  statistics and econometrics to measure events that happen over a period of \n",
      "time. The model is used to understand past data or predict future data in a series.  \n",
      "ARIMA models are applied in some cases where data show evidence of  non-\n",
      "stationarity  in the sense of mean (but not variance/ autocovariance ), where an initial \n",
      "differencing step (corresponding to the  \"integrated \" part of the model) can be applied \n",
      "one or more times to eliminate the non -stationarity of the mean function (i.e., the trend).  \n",
      "Seasonal Autoregressive Integrated Moving Average,  SARIMA or Seasonal ARIMA, \n",
      "is an extension of ARIMA that explicitly supports univariate time series data with a \n",
      "seasonal component. A s easonal ARIMA model is formed by including additional \n",
      "seasonal terms in the ARIMA.  \n",
      "It adds three new hyperparameters to specify the autoregression (AR), differencing (I) \n",
      "and moving average (MA) for the seasonal component of the series, as well as an \n",
      "additi onal parameter for the period of the seasonality.  \n",
      " \n",
      "How do you check the stationarity of the time series?  \n",
      " \n",
      "The simplest  way to check for stationarity is  to split your total timeseries into 2, 4, or \n",
      "10 (say N) sections  (the more the better), and compute the mean and variance within \n",
      "each section. If there is an obvious trend in either the mean or variance over the N \n",
      "sections , then your series is not stationary.  \n",
      "Hypothesis testing?  \n",
      "The Hypothesis Testing  is a statistical  test used to determine whether \n",
      "the hypothesis  assumed for the sample of data stands true for the entire population or \n",
      "not. Simply, the  hypothesis  is an assu mption which is  tested  to determine the \n",
      "relationship between two data sets.  \n",
      "The types of hypotheses  testing : \n",
      " Simple Hypothesis.  \n",
      " Complex Hypothesis.  \n",
      " Working or Research Hypothesis.  \n",
      " Null Hypothesis.  \n",
      " Alternative Hypothesis.  \n",
      " Logical Hypothesis.  \n",
      " Statistical Hypothesis.  \n",
      "Data normalization and data standardization? And which one is prone to outliers?  \n",
      "Normalization typically means  rescales the values into  a range of [0,1]. \n",
      "Standardization typically means rescales data to have a mean of 0 and a standard \n",
      "deviati on of 1 (unit variance).  \n",
      "Normalizing  the data is  sensitive to outliers, so if there are  outliers  in the data set it is \n",
      "a bad practice.  Standardization  creates a new data not bounded \n",
      "(unlike  normalization).  \n",
      "How back propagation works?  \n",
      "Back -propagation is just a way of propagating the total loss back into the neural \n",
      "network to know how much of the loss every node is responsible for, and subsequently \n",
      "updating the weights in such a way that minimizes the loss by giving the nodes with \n",
      "higher error rates lower weights and vice versa.  \n",
      " \n",
      "Backpropagation algorithm works by  computing the gradient of the loss function with \n",
      "respect to each weight by the chain rule, computing the gradient one layer at a time.  \n",
      "What is Vanishing and Exploding gradients?  \n",
      "Vanishing: As the backpropagation algorithm advances downwards ( or backward) \n",
      "from the output layer towards the input layer, the gradients often get smaller and \n",
      "smaller and approach zero which eventually leaves the weights of the initial or lower \n",
      "layers nearly unchang ed. As a result, the gradient descent never converges to the \n",
      "optimum. This is known as the  vanishing gradients  problem.  \n",
      "Exploding: On the contrary, in some cases, the gradients keep on getting larger and \n",
      "larger as the backpropagation algorithm progresses. This, in turn, causes very large \n",
      "weight updates and causes the gradient descent to diverge. This is known as \n",
      "the exploding gradients  problem.  \n",
      "How do you overcome vanishing and Exploding gradients?  \n",
      "Vanishing Gradients: The simplest solution is to use other activation functions, such \n",
      "as ReLU, which doesn't cause a small derivative. Residual networks are another \n",
      "solution, as they provide residual connections straight to earlier layers.  \n",
      "Exploding Gradients : A common solutio n to exploding gradients is to  change the error \n",
      "derivative before propagating it backward through the network and using it to update \n",
      "the weights. By rescaling the error derivative, the updates to the weights will also be \n",
      "rescaled, dramatically decreasing t he likelihood of an overflow or underflow.  \n",
      "How do you initialise weights to NN? And explain Adagrad and Adam intializers?  \n",
      "Neural network models are fit using an optimization algorithm called stochastic \n",
      "gradient descent that incrementally changes the network weights to minimize a loss \n",
      "function, hopefully resulting in a set of weights for the mode that is capable of making \n",
      "useful predictions.  \n",
      "This optimization algorithm requires a starting point in the space of possible weight \n",
      "values from which to begin  the optimization process. Weight initialization is a \n",
      "procedure to set the weights of a neural network to small random values that define \n",
      "the starting point for the optimization (learning or training) of the neural network model.  \n",
      "Each time, a neural networ k is initialized with a different set of weights, resulting in a \n",
      "different starting point for the optimization process, and potentially resulting in a \n",
      "different final set of weights with different performance characteristics.  \n",
      "We cannot initialize all weigh ts to the value 0.0 as the optimization algorithm results in \n",
      "some asymmetry in the error gradient to begin searching effectively.  \n",
      "He-init and xavier Initialization differences?  \n",
      "The main difference for machine learning practitioners is the following:  \n",
      " He initialization works better for layers with ReLu activation.  \n",
      " Xavier initialization works better for layers with sigmoid activation.  \n",
      "Why CNN for images?  \n",
      "CNNs are  used for image classification and recognition because of its high accuracy. \n",
      "The CNN follows a hierarchical model which works on building a network, like a funnel, \n",
      "and finally gives out a fully -connected layer where all the neurons are connected to \n",
      "each other and the output is processed.  \n",
      "How back propagation works in max pooling layer of cnn ? \n",
      "For the backward in a max pool layer, we pass of the gradient, we start with a zero \n",
      "matrix and fill the max index of this matrix with the gradient from above. On the other \n",
      "hand, if we tread it as an average pool layer, we need to fill each cell with the value of \n",
      "the gradient from above.  \n",
      "How do you train object detection model and deploy it?  \n",
      "Train Object Detection Model:  \n",
      "1. Collect your datasets  \n",
      "2. Annotate the custom images using ‘labelImg’  \n",
      "3. Split them into train -test sets  \n",
      "4. Generate a TFRecord for the train -test split  \n",
      "5. Setup a config file  \n",
      "6. Train the actual model  \n",
      "7. Export the graph from the newly trained model  \n",
      "8. Bring in the frozen_inference_graph to classify in real -time \n",
      "How do you check the accuracy of ocr output?  \n",
      "Measuring OCR accuracy is done by  taking the output of an OCR run for an image \n",
      "and comparing it to the original version of the same text. You can then either count \n",
      "how many characters were detected correctly (character level accuracy), or count how  \n",
      "many words were recognized correctly (word level accuracy).  \n",
      "Brief on Transformers architecture and Attention models.  \n",
      "Transformers are a type of neural network architecture that have been gaining \n",
      "popularity .  \n",
      "Transformers were recently used by OpenAI in  their language  models , and also used \n",
      "recently by DeepMind for  AlphaStar  — their program to defeat a top professional \n",
      "Starcraft player.  \n",
      "Transformers were developed to solve the problem of  sequence  transduction , or neural \n",
      "machine translation.  That means any task that transforms an input sequence to an \n",
      "output sequence. This includes speech recognition, text -to-speech transformation, etc.  \n",
      " \n",
      "Attention models, or attention mechanisms, are  input processing techniques for neural \n",
      "networks that allows the network to focus on specific aspects  of a complex input, one \n",
      "at a time until the entire dataset is categorized. Attention models require continuous \n",
      "reinforcement or backpopagation training to be effective.  \n",
      "Decorators and Iterators in python  \n",
      "The python generators give an easy way of creating iterators. These generators \n",
      "instead of returning the function from the return statement use the  \"yield”  keyword. \n",
      "These are the generator version of the list comprehensions.  \n",
      "If the function contains at least one “yield” statement, it becomes a gene rator function. \n",
      "Both the  yield  and return  will return some value from the function.  \n",
      "we can implement decorators’  concept in two ways: Class decorators. Function \n",
      "decorators. Usually, a decorator is  any callable object that is used to modify the \n",
      "function (or) the class.  \n",
      " \n",
      "Iterators  are used mostly to iterate or convert other objects to an iterator  using iter() \n",
      "function . Generators are mostly used in loops to generate an iterator by returning all \n",
      "the values in the loop without affecting the iteration of the loop. Iterator uses iter() and \n",
      "next() functions.  \n",
      "Why can’t we use traditional machine learning algorithms for Time series?  \n",
      "Time series forecasting is an important area of machine learning. It is important \n",
      "because there are so many prediction problems that involve a time component. \n",
      "However, while the time component adds additional information, it also makes time \n",
      "series problems more difficult to handle compared to many other prediction tasks. \n",
      "Time series data, as the name indicates, differ from other types of data in the sense \n",
      "that the temporal aspect is important. On a positive note, this gives us additional \n",
      "inform ation that can be used when building our machine learning model — that not only \n",
      "the input features contain useful information, but also the changes in input/output over \n",
      "time.  \n",
      " \n",
      "Comparing the performance of all methods, it was found that the machine learning  \n",
      "methods were all out -performed by simple classical methods, where  ETS and ARIMA \n",
      "models  performed the best overall. This finding confirms the results from previous \n",
      "similar studies and competitions.  \n",
      " \n",
      "Why Vector Auto regression over LSTM’s?  \n",
      "Vector autoregression  (VAR) is a statistical model used to capture the relationship \n",
      "between multiple quantities as they change over time. VAR is a type of  stochastic \n",
      "process  model. VAR models generalize the single -variable (univariate)  autoregressive \n",
      "model  by allowing for multivariate  time series . VAR models are often used \n",
      "in economics  and the  natural sciences . \n",
      "VAR MODELING  \n",
      "With ARIMA we are using the past values of every variable to make the predictions for \n",
      "the future. When we have multiple time series at our disposal, we can also extract \n",
      "information from their relationships, in this way VAR is a multivariate generalization of \n",
      "ARIMA because it understands and uses the relationship between several inputs. This \n",
      "is useful for describing the dynamic behavior of the data and also provides better \n",
      "forecasting results.  \n",
      "To correctly develop a VAR model, the same classical assumptions encountered when \n",
      "fitting an ARIMA, have to be satisfied. We need to grant stationarity and leverage \n",
      "autocorrelation behaviors. These prerequisites enable us to develop a stable model. All \n",
      "our time series are stationary in mean and show a daily and weekly pattern.  \n",
      "COMBINE VAR AND LSTM  \n",
      "Now our scope is to use our fitted VAR to improve the training of our neural network. \n",
      "The VAR has learned the internal behavior of our multivariate data source ad justing the \n",
      "insane values, correcting the anomalous trends, and reconstructing properly the NaNs.  \n",
      " Our strategy involves applying a two -step training procedure. We start feeding our \n",
      "LSTM autoencoder, using the fitted values produced by VAR, for multi -step ahead \n",
      "forecasts of all the series at our disposal (multivariate output). Then we conclude the \n",
      "training with the raw data, in our case they are the same data we used before to fit the \n",
      "VAR. With our neural network, we can also combine external data sources, for \n",
      "example, the weather conditions or some time attributes like weekdays, hours, and \n",
      "months that we cyclically encode.  \n",
      "We hope that our neural network can learn from two different but similar data sources \n",
      "and perform better on our test data. When performi ng multiple -step training we have to \n",
      "take care of the  Catastrophic Forgetting  problem.  Catastrophic forgetting is a problem \n",
      "faced by many models and algorithms. When trained on one task, then trained on a \n",
      "second task, many machine learning models “forget” how to perform the first task. This \n",
      "is widely believed to be a serious problem for neur al networks.  \n",
      "To avoid this tedious problem, the structure of the entire network has to be properly \n",
      "tuned to provide a benefit in performance terms. From these observations, we preserve \n",
      "a final part of our previous training as validation.  \n",
      "Technically speaki ng the network is very simple. It’s constituted by a seq2seq LSTM \n",
      "autoencoder which predicts the available sensors N steps ahead in the future. The \n",
      "training procedure is carried out using  keras -hypetune . This framework \n",
      "provides  hyperparameter optimization  of the neural network structures in a very \n",
      "intuitive way. This is done for all three training involved (the fit on VAR fitted values, the \n",
      "fine-tuning fit with the raw data and the standard fit di rectly on the raw data)  \n",
      " \n",
      "1. What are the steps that you have followed in your last project to prepare the \n",
      "dataset?  \n",
      "Answer:  The choice of data entirely depends on the problem you’re trying to solve \n",
      "but, in my case, I have followed these 5 steps to prepare the dataset;  \n",
      "Step1: Gathering the data  \n",
      "The quantity of the data is important but not as important as the quality of it.  \n",
      "Step2: Handling missing data  \n",
      "This is one of the hardest step and h andling missing data in the wrong way can \n",
      "cause disasters.  \n",
      "Step3: Taking data further with  the feature extraction  \n",
      "Feature extraction can be a turning point. It is what makes a dataset unique. Getting \n",
      "insight by making relations between features is important thing.  \n",
      "Step4: Deciding which key factors are important  \n",
      "AI is able to decide which featur es truly affect the output and which doesn’t. On the \n",
      "downside, The more data you give your model, it costs you money (computer \n",
      "power) & time. Both not always available. So, Giving your program a little help isn’t \n",
      "always a bad idea. If you’re sure that a ce rtain feature is completely unrelated to \n",
      "the output, you should just disregard it altogether.  \n",
      "Step5: Splitting the data into training & testing sets  \n",
      "The data is 80 –20 percent training & testing sets respectively. The 20% for the test \n",
      "set has engineered in a way that they’re not just randomly cut out of the dataset.  \n",
      "2. In your last project what steps were involved in model selection procedure?  \n",
      "Answer:  As considering the data -rich situation, the approach is selected where we \n",
      "randomly divide the dataset into thre e parts: training set, validation set, and, test \n",
      "set. Where training set was used to fit the models; the validation set was used to \n",
      "estimate the prediction error for model selection; and finally, the test set was used \n",
      "for assessment of the generalization e rror of the final chosen model.  \n",
      "We used k -fold cross -validation that splits the training dataset into k folds , where \n",
      "each example appears in a test set only once. At last, we have finalized the model \n",
      "based on test data score.  \n",
      "3. If I give you 2 columns of any  dataset, what will be the steps will be involved to \n",
      "check the relationship between those 2 columns?  \n",
      "Answer:  The first step will be to check the columns contain which kind of data type \n",
      "such as; Continuous  or categorical.  So that we can check correlation be tween \n",
      "categorical and numerical variables.  \n",
      " \n",
      " \n",
      "Can you please explain 5 diff kind of strategies at least to handle missing values \n",
      "in dataset?  \n",
      "\n",
      "Answer:  \n",
      "1. Deleting Rows with missing values: Missing values can be handled by \n",
      "deleting the rows or columns having null values. If columns have more than \n",
      "half of rows as null then the entire column can be dropped. The rows which \n",
      "are having one or more columns values as null can also be dropped.  \n",
      "2. Impute missing values with mean/median: Columns in the dataset which are \n",
      "having numeric continuous values can be replaced with the mean, median, \n",
      "or mode of remaining values in the column. Replacing the above two \n",
      "approximations (mean, median) is a statistical approach to handle the \n",
      "missing values.  \n",
      "3. Impute missing values for categor ical variable : When missing values is from \n",
      "categorical columns (string or numerical) then the missing values can be \n",
      "replaced with the most frequent category. If the number of missing values is \n",
      "very large then it can be replaced with a new category.  \n",
      "4. Missing  values  imputation using k -NN:: The k nearest neighbours is an \n",
      "algorithm that is used for simple classification. The algorithm uses ‘feature \n",
      "similarity’ to predict the values of any new data points.  \n",
      "5. Imputation using Deep Learning Library (Datawig): This me thod works very \n",
      "well with categorical and non -numerical features. It is a library that learns \n",
      "Machine Learning models using Deep Neural Networks to impute missing \n",
      "values in a dataframe. It also supports both CPU and GPU for training.  \n",
      "What kind of diff. iss ues you have faced wrt your raw data? At least mention 5 \n",
      "issues.  \n",
      "Answer:   \n",
      "1. Getting data from multiple sources  \n",
      "2. Unlocking value out of Unstructured Text Data  \n",
      "3. Setting up the infrastructure and velocity of data  \n",
      "4. Adapting to different tools to collect unstructure d data  \n",
      "5. Building a robust strategy before collecting data  \n",
      " \n",
      "What is your strategy to handle categorical dataset? Explain with example.  \n",
      "Answer:  Categorical features have a lot to say about the dataset thus it should be \n",
      "converted to numerical to make it into a  machine -readable format.  \n",
      "Two major types of categorical features are  \n",
      " Nominal  – These are variables which are not related to each other in any order \n",
      "such as colour (black, blue, green).  \n",
      " Ordinal  – These are variables where a certain order can be found between them \n",
      "such as student grades (A, B, C, D, Fail).  \n",
      "Encoding Categorical Variables is main approach to handle  categorical dataset.  \n",
      " \n",
      "How do you define a model in terms of machine learning or in your own word?  \n",
      "Answer:  Machine learning is the study of computer algorithms that improve \n",
      "automatically through experience and by the use of data. It is seen as a part of \n",
      "artificial intelligence. Machine learning algorithms build a model based on sample \n",
      "data, known as \"training data\", in order to make predictions or decisions without \n",
      "being explicitly programmed to do so.  \n",
      "What do you understand by k fold validation & in what situation you have used \n",
      "k fold cross validation?  \n",
      "Answer:  We used k -fold cross -validation that splits the tr aining dataset into k folds, \n",
      "where each example appears in a test set only once. At last, we have finalized the \n",
      "model based on test data score in model selection procedure.  \n",
      "What is meaning of bootstrap sampling? explain me in your own word.  \n",
      "Answer:  Bootstr ap Sampling is a method that involves drawing of sample data \n",
      "repeatedly with replacement from a data source to estimate a population \n",
      "parameter.  \n",
      "Bootstrap sampling is used in a machine learning ensemble algorithm called \n",
      "bootstrap aggregating (also called ba gging). It helps in avoiding overfitting and \n",
      "improves the stability of machine learning algorithms.  \n",
      "In bagging, a certain number of equally sized subsets of a dataset are extracted \n",
      "with replacement. Then, a machine learning algorithm is applied to each of these \n",
      "subsets and the outputs are ensembled.  \n",
      "What do you understand by underfitting & overfitting of model with example?  \n",
      "Answer:  The situation where any given model is performing too well on the training \n",
      "data but the performance drops significantly over th e test set is called an overfitting \n",
      "model.  \n",
      "For example, non -parametric models like decision trees, KNN, and other tree -\n",
      "based algorithms are very prone to overfitting. These models can learn very \n",
      "complex relations which can result in overfitting.  \n",
      " \n",
      "On the other hand, if the model is performing poorly over the test and the train set, \n",
      "then we call that an underfitting model. An example of this situation would be \n",
      "building a linear regression model over non -linear data.  \n",
      "What is diff between cross validation and  bootstrapping?  \n",
      "Answer: Bootstrapping is a technique that helps in many situations like validation \n",
      "of a predictive model performance, ensemble methods, estimation of bias and \n",
      "variance of the model. It works by sampling with replacement from the original da ta, \n",
      "and take the “not chosen” data points as test cases. We can make this several \n",
      "times and calculate the average score as estimation of our model performance.  \n",
      "In addition, Bootstrapping helps in ensemble methods as we may build a model \n",
      "(like a Decision tr ee) using each bootstrap data set and “bag” these models in an \n",
      "ensemble (like Random Forest) and take the majority voting for all of these models \n",
      "as our resulting classification.  \n",
      "On the other hand, cross validation is a technique for validating the model \n",
      "performance, and it’s done by split the training data into k parts. We take k -1 parts \n",
      "as our training set and use the “held out” part as our test set. We repeat that k times \n",
      "differently (we hold out different part every time). Finally we take the average of  the \n",
      "k scores as our performance estimation.  \n",
      " \n",
      "Cross validation can suffer bias or variance. if we increase the number of splits (k), \n",
      "the variance will increase and bias will decrease. On contrast, if we decrease (k), \n",
      "the bias will increase and variance will decrease. Generally 10 -fold CV is used but \n",
      "of course it depends on the size of the training data.  \n",
      "What do you understand by silhouette coefficient?  \n",
      "Answer:  Silhouette Coefficient or silhouette score is a metric used to calculate the \n",
      "goodness of a clustering technique. Its value ranges from -1 to 1.  \n",
      "1: Means clusters are well apart from each other and clearly distinguished.  \n",
      "\n",
      "0: Means clusters are indifferent, or we can say that the distance between clusters \n",
      "is not significant.  \n",
      "-1: Means clusters are assigned in the wrong way.  \n",
      " \n",
      "Silhouette Score = (b -a)/max(a,b)  \n",
      "Where,  \n",
      "a= average intra -cluster distance i.e the average distance between each point \n",
      "within a cluster.  \n",
      "b= average inter -cluster distance i.e the average distance between all clusters.  \n",
      "What is the advantage of using ROC Score?  \n",
      "Answer:   \n",
      " A simple graphical representation of the diagnostic accuracy of a test: the closer \n",
      "the apex of the curve toward the upper left corner, the greater the discriminatory \n",
      "ability of the test.   \n",
      " Allows a simple graphical comparison between diagnostic tests  \n",
      " Allows a simple  method of determining the optimal cut -off values, based on \n",
      "what the practitioner thinks is a clinically appropriate (and diagnostically \n",
      "valuable) trade -off between sensitivity and false positive rate.  \n",
      " Also, allows a more complex (and more exact) measure o f the accuracy of a \n",
      "test, which is the AUC  \n",
      "o The AUC in turn can be used as a simple numeric rating of diagnostic \n",
      "test accuracy, which simplifies comparison between diagnostic tests.  \n",
      "o The AUC is non -parametric, which means it is unaffected by abnormal \n",
      "distrib utions in the population  \n",
      "Explain me complete approach to evaluate your regression model  \n",
      "\n",
      "Answer:  There are 3 main metrics for model evaluation in regression:  \n",
      "1. R Square/Adjusted R Square : R Square measures how much variability in \n",
      "dependent variable can be explained by the model. It is the square of the \n",
      "Correlation Coefficient(R) and that is why it is called R Square.  \n",
      " \n",
      " Mean Square Error(MSE)/Root Mean Square Error(RMSE) : While R Square is a \n",
      "relative measure of how well the model fits dependent variables, Mean Square \n",
      "Error is an absolute measure of the goodness for the fit.  \n",
      "Root Mean Square Error(RMSE) is the square root of MSE. It is used more \n",
      "commonly than MSE because firstly sometimes MSE value can be too big to \n",
      "compare easily. Secondly, MSE is calculated by the square of error, and thus \n",
      "square root brings it back to the same level of prediction error and makes it easier \n",
      "for interpretation.  \n",
      " \n",
      "3. Mean Absolute Error(MAE ): Mean Absolute Error(MAE) is similar to Mean \n",
      "Square Error(MSE). However, instead of the sum of square of error in MSE, MAE \n",
      "is taking the sum of the absolute value of error.  \n",
      " \n",
      "Give me example of lazy learner and eagar learner algorithms example.  \n",
      "Answer:   \n",
      "Lazy learner:  \n",
      "1. Just store Data set  without  learning from it  \n",
      "\n",
      "2. Start classifying data when it receive  Test data  \n",
      "3. So it takes less time learning and more time classifying data  \n",
      "Eager learner:  \n",
      "1. When it receive data set it starts classifying (learning)  \n",
      "2. Then it does not wait for test data to learn  \n",
      "3. So it takes long time learning and less time classifying data  \n",
      "In supervised learning Some examples are :  \n",
      "Lazy  : K - Nearest Neighbour, Case - Based Reasoning  \n",
      "Eager  : Decision Tree, Naive Bayes, Artificial Neural Networks  \n",
      "Wha t do you understand by holdout method?  \n",
      "Answer:  Holdout Method is the simplest sort of method to evaluate a classifier. In \n",
      "this method, the data set (a collection of data items or examples) is separated into \n",
      "two sets, called the Training set and Test set.  \n",
      "A classifier performs function of assigning data items in a given collection to a target \n",
      "category or class.  \n",
      "Example –E-mails in our inbox being classified into spam and non -spam.  \n",
      "Classifier should be evaluated to find out, it’s accuracy, error rate, and err or \n",
      "estimates. It can be done using various methods. One of most primitive methods \n",
      "in evaluation of classifier is ‘Holdout Method’.  \n",
      "In the holdout method, data set is partitioned, such that – maximum data belongs \n",
      "to training set and remaining data belongs to test set.  \n",
      "What is diff between predictive modelling and descriptive modelling.  \n",
      "Answer:  \n",
      " \n",
      "How you have derived a feature for model building in your last project?  \n",
      "Answer:  The great features that describe the structures inherent in your data.  \n",
      "Better featur es means flexibility and Better features means simpler models.  \n",
      "Tabular data is described in terms of observations or instances (rows) that are \n",
      "made up of variables or attributes (columns). An attribute could be a feature.  \n",
      "The idea of a feature, separate fr om an attribute, makes more sense in the context \n",
      "of a problem. A feature is an attribute that is useful or meaningful to your problem. \n",
      "It is an important part of an observation for learning about the structure of the \n",
      "problem that is being modeled.  \n",
      " \n",
      "I use “ meaningful” to discriminate attributes from features. Some might not. I think \n",
      "there is no such thing as a non -meaningful feature. If a feature has no impact on \n",
      "the problem, it is not part of the problem.  \n",
      "\n",
      "Explain 5 different encoding techniques.  \n",
      "Answer:  Since most machine learning models only accept numerical variables, \n",
      "preprocessing the categorical variables becomes a necessary step. We need to \n",
      "convert these categorical variables to numbers such that the model is able to \n",
      "understand and extract valuable info rmation.  \n",
      "1. Label Encoding or Ordinal Encoding: We use this categorical data encoding \n",
      "technique when the categorical feature is ordinal. In this case, retaining the \n",
      "order is important. Hence encoding should reflect the sequence. In Label \n",
      "encoding, each label is converted into an integer value. We will create a variable \n",
      "that contains the categories representing the education qualification of a \n",
      "person.  \n",
      "2. One hot Encoding: We use this categorical data encoding technique when the \n",
      "features are nominal(do not have any  order). In one hot encoding, for each level \n",
      "of a categorical feature, we create a new variable. Each category is mapped \n",
      "with a binary variable containing either 0 or 1. Here, 0 represents the absence, \n",
      "and 1 represents the presence of that category. These newly created binary \n",
      "features are known as Dummy variables.  \n",
      "3. Dummy Encoding: Dummy coding scheme is similar to one -hot encoding. This \n",
      "categorical data encoding method transforms the categorical variable into a set \n",
      "of binary variables (also known as dummy va riables). In the case of one -hot \n",
      "encoding, for N categories in a variable, it uses N binary variables. The dummy \n",
      "encoding is a small improvement over one -hot-encoding. Dummy encoding \n",
      "uses N -1 features to represent N labels/categories.  \n",
      "4. Binary Encoding: Bina ry encoding is a combination of Hash encoding and one -\n",
      "hot encoding. In this encoding scheme, the categorical feature is first converted \n",
      "into numerical using an ordinal encoder. Then the numbers are transformed in \n",
      "the binary number. After that binary value is split into different columns. Binary \n",
      "encoding works really well when there are a high number of categories. For \n",
      "example the cities in a country where a company supplies its products.  \n",
      "5. Target Encoding: In target encoding, we calculate the mean of the targ et \n",
      "variable for each category and replace the category variable with the mean \n",
      "value. In the case of the categorical target variables, the posterior probability of \n",
      "the target replaces each category.  \n",
      "4. How do you define some features are not important for ML model? What \n",
      "strategy will you follow  \n",
      "Answer:  Unnecessary features decrease training speed, decrease model \n",
      "interpretability, and, most importantly, decrease generalization performance on the \n",
      "test set.  \n",
      "The FeatureSelector library can be used to select import ant features.  \n",
      "The most common feature selection methods:  \n",
      " Features with a high percentage of missing values: The first method for \n",
      "finding features to remove is straightforward: find features with a fraction of \n",
      "missing values above a specified threshold.  \n",
      " Collinear (highly correlated) features: Collinear features are features that \n",
      "are highly correlated with one another. In machine learning, these lead to \n",
      "decreased generalization performance on the test set due to high variance \n",
      "and less model interpretabilit y. \n",
      " Features with zero importance in a tree -based model : It finds features that \n",
      "have zero importance according to a gradient boosting machine (GBM) \n",
      "learning model.  \n",
      " Features with low importance : The function identify_low_importance finds \n",
      "the lowest importanc e features that do not contribute to a specified total \n",
      "importance. For example, the call below finds the least important features \n",
      "that are not required for achieving 99% of the total importance:  \n",
      " Features with a single unique value : A feature with only one unique value \n",
      "cannot be useful for machine learning because this feature has zero \n",
      "variance. For example, a tree -based model can never make a split on a \n",
      "feature with only one value (since there are no groups to divide the \n",
      "observations into).  \n",
      " \n",
      "List down at least 5 vectorization technique.   \n",
      " 5 vectorization techniques are - \n",
      "1. Bag of Words.  \n",
      " BOW is a representation of text that describes the occurrence of words within a   \n",
      "document . We just keep track of word counts and disregard the grammatical details \n",
      "and the word order. It is called a “bag” of words because any information about the \n",
      "order or structure of words in the document is discarded.  \n",
      "2.  TF  -IDF \n",
      " TF stands for Term Frequency.  \n",
      "  \n",
      "IDF stands for Inverse Document Frequency . Where DF - Document Frequency.  \n",
      " \n",
      " \n",
      "\n",
      " \n",
      "3. Word2Vec  \n",
      " Word2Vec used to group the vectors of similar words together in vector -space .  It \n",
      "uses simple Neural -Networks for word -embeddings.  \n",
      "To implement Word2Vec we use two techniques -  \n",
      "I) Skip -Gram , II)  CBOW( Continuous Bag of Words.)  \n",
      " \n",
      "4. GloVe  \n",
      " It is Similar to Word2Vec but GloVe is also creating contextual word embeddings \n",
      "but given the great performance of Word2Vec . \n",
      "5. FastTex  \n",
      " It was introduced by Facebook in 2016.  It is also very much similar to Word2Vec. \n",
      "But it has the capability of generalizing the  unknown words, which other methods  can \n",
      "miss . \n",
      " What is difference between RNN and Encoder -Decoder?  \n",
      "  \n",
      " A recurrent neural network (RNN) is a type of artificial neural network which \n",
      "uses sequential data or time series data. It can remembers its input, due to an \n",
      "internal memory.  \n",
      " Encoder is a stack of several recurrent units where each accepts a single \n",
      "element  of the input sequence, collects information for that element and \n",
      "propagates it forward . \n",
      " Where Decoder is also a  stack of several recurrent units where each predicts an \n",
      "output at a particular time step. each recurrent unit accepts a hidden state from \n",
      "the previous unit and produces and output as well as its own hidden state  \n",
      " \n",
      " \n",
      "What do you understand by attention  mechanism and what is use of it ?  \n",
      " In deep learning attention mechanism is an attempt to implement perform a \n",
      "particular action by concentrating a few relevant things, while ignoring the other neural \n",
      "networks.  The attention mechanism also brought an improvement over the encoder \n",
      "decoder -based  on neural machine translation system in NLP . It helps to  memorize \n",
      "long source sentences in neural machine translation (NMT). Rather than building a \n",
      "single context vector out of the encoder's last hidden state . It was used in other \n",
      "applications, including  Computer Vision , speech processing, et c. \n",
      " \n",
      "Have you read a research paper Attention you all need? If not, then why you are \n",
      "claiming you know NLP?  \n",
      " I have read it.  From that research paper, I understood how attention mechanism is \n",
      "affecting encoder -decoder process in neural machine translation system in NLP .  \n",
      "What do you understand by multi headed attention? Explain.  \n",
      " Multi headed attention  is a mechanisms which runs through an attention \n",
      "mechanism several times in parallel. The independent attention outputs are then \n",
      "concatenated and linearly transformed into the expected dimension.  \n",
      " Basically, multiple attention heads repeats its computations multiple times in parallel. \n",
      "Each of these is called an Attention Head. The Attention module splits its Query, Key, \n",
      "and Value parameters in multiple ways and passes each split independently through a \n",
      "separate Head. All of these calculations are then combined together to p roduce a final  \n",
      "attention output . It gives the t ransformer greater power to encode multiple relationships  \n",
      "for each word.   \n",
      "\n",
      " \n",
      "Tell me something about your project you have done in past?   \n",
      "  In the past I have done  several projects from them one of them is,  Number plate \n",
      "Detection. In that project, I have used data from Google Open Image V6 images  with \n",
      "OIDV toolkit & trained using YOLOv4 -tiny (for real time detection.)  & implemented as \n",
      "a local  desktop app with flask  API & html.  \n",
      "What was your Dataset size for ML Project?   \n",
      " For a ML based project of Flight -Price -Prediction model I have used a dataset with \n",
      "10684 rows . \n",
      "What is type of your dataset?   \n",
      " For Flight -Price -Prediction model dataset type was a combination of numerical and \n",
      "string data.  \n",
      "What was frequency of your dataset? (E.g. batch, streaming etc) ? \n",
      "  In this dataset, I have used k -fold Cross Validation with 5 folds & used Xgboost \n",
      "algorithm.  \n",
      "What was source system for your dataset? (E.g. sensor, satellite Kafka, cloud, etc.) . \n",
      "What was kind of derived dataset that you have mentioned in project?   \n",
      " As a derived dataset, I have u sed credit card fraud detection data which is a  \n",
      "imbalance dataset problem.  \n",
      "How you have done validation dataset?   \n",
      " In Machine learning solutions for robust solution I have used k -fold. If there is n \n",
      "number of folds then, it used (n -1) is for training & the last fold data is for validation.  \n",
      "Have you c reated any pipeline to validate  this dataset or you were using any tool?   \n",
      " For Machine learning solutions I have created pipelines using scikit learn . \n",
      "What do you understand by data lake?   \n",
      "\n",
      " Data lakes are next -generation  data management solutions  that can help \n",
      "businesses data scientists in meet big data challenges and drive new levels of real -\n",
      "time analytics.  It is an eas y accessible, centralized storage repository for  large \n",
      "volumes of structured and unstructured  for hosting raw, unprocess ed enterprise data . \n",
      "What do you understand by data warehousing?   \n",
      "  Data warehouse is  a system used for storing and reporting on data . It is basically, \n",
      "data warehousing is an electronic method of organizing, analyzing, and reporting \n",
      "information. In modern business, being able to integrate multiple sources of data is \n",
      "crucial to make better -informed decisions .  \n",
      "A data warehouse essentially combines information from several sources into one \n",
      "comprehensive database.  \n",
      "Can you please name some validation s that you have done on top of your data?   \n",
      " K-fold, k -means Clustering, Train -test-split etc.  \n",
      "How you have handled streaming dataset?   \n",
      " I have handled streaming dataset by taking it as a form of batch & storing it in a \n",
      "database.  \n",
      "How many different types of environments were available in your project?   \n",
      " For Number plate detection, I have used  multiple environments like - TFOD, \n",
      "OpenCV, Yolo etc. For, different projects, I have used different environments.  \n",
      "What was your delivery mechanism for particular project?  \n",
      " For delivery generally, I use Flask API & OpenCV with HTML & CSS implementation \n",
      "as a local/cloud desktop app.   \n",
      "Have you used any OPS pipeline for this current project?  \n",
      " NO. \n",
      "How y ou were doing model retraining?  \n",
      " Using proper pipeline with Py -charm & HTML.  \n",
      "How you have implemented model retraining in your project?   \n",
      " I have used sequence of pipeline so that, whenever it can automatically done all the \n",
      "pre-training processes needed and when start tr aining it will replace the old model with \n",
      "the new one & can do prediction.  \n",
      "How frequently you have been doing model retraining and what was the strategy for \n",
      "model retraining?  \n",
      " For streaming, I am doing model retraining in every 15 days with proper pip eline.  \n",
      "How you can define Machine Learning?  \n",
      "Machine learning is the concept that a computer program can learn and adapt to new \n",
      "data without human intervention. Machine learning is a field of artificial intelligence (AI) \n",
      "that keeps a computer’s built -in algorithms current regardless of changes in the \n",
      "worldwide economy.  \n",
      "Or \n",
      "Machine Learning is the science of getting computers to learn and act like humans do, \n",
      "and improve their learning over time in autonomous fashion, by feeding them data and \n",
      "information in the form of observations and real -world interactions  \n",
      " \n",
      "What do you understand Labelled training dataset?  \n",
      "Labelled training set is a set of training data which has a solution to the problem or \n",
      "task (a.k.a. label). Labelled data is a designation for pieces of data that have been \n",
      "tagged with one or more labels identifying certain properties or characteristics, or \n",
      "classifications or contained objects. Labels make that data specifically useful in certain \n",
      "types of machine learning known as supervised machine lea rning setups.  Labelled \n",
      "dataset is the process of identifying raw data (images, text files, videos, etc.) and \n",
      "adding one or more meaningful and informative labels to provide context so that a \n",
      "machine learning model can learn from it.  \n",
      "What are 2 most comm on supervised ML tasks you have performed so far?  \n",
      "Two most common supervised tasks are classification and regression  \n",
      "What kind of Machine learning algorithm would you used to walk robot in various \n",
      "unknown area?  \n",
      "The best Machine Learning algorithm to all ow a robot to walk in unknown terrain is \n",
      "Reinforced Learning, where the robot can learn from response of the terrain to optimize \n",
      "itself.  \n",
      "What kind of ML algo you can use to segment your user into multiple groups?  \n",
      "The best algorithm to segment customers into multiple groups is either supervised \n",
      "learning (if the groups have known labels) or unsupervised learning (if there are no \n",
      "group labels).  \n",
      "What type of learning algo realised on similarity measure to make a prediction?  \n",
      "Learning algorithm that relies on a similarity measure to make predictions is instance -\n",
      "based algorithm.  \n",
      "What is an online learning system?  \n",
      " \n",
      "Online learning system is a learning system in which the machine learns as data is \n",
      "given in small streams continuously. In computer science, online machine learning is \n",
      "a method of machine learning in which data becomes available in a sequential order \n",
      "and is used to update the best predictor for future data at each step, as opposed to \n",
      "batch learning techniques which generate the best predictor b y learning on the entire \n",
      "training data set at once. Online learning is a common technique used in areas of \n",
      "machine learning where it is computationally infeasible to train over the entire dataset, \n",
      "requiring the need of out -of-core algorithms. It is also us ed in situations where it is \n",
      "necessary for the algorithm to dynamically adapt to new patterns in the data, or when \n",
      "the data itself is generated as a function of time, e.g., stock price prediction. Online \n",
      "learning algorithms may be prone to catastrophic int erference, a problem that can be \n",
      "addressed by incremental learning approaches.  \n",
      "What is out of core learning?  \n",
      "Out-of-core learning system is a system that can handle data that cannot fit into your \n",
      "computer memory. It uses online learning system to feed d ata in small bits. Out-of-\n",
      "core learning refers to a set of algorithms working with data that cannot fit into the \n",
      "memory of a single computer, but that can easily fit into some data storage such as a \n",
      "local hard disk or web repository. Your available RAM, th e core memory on your single \n",
      "machine, may indeed range from a few gigabytes (sometimes 2 GB, more commonly \n",
      "4 GB, but we assume that you have 2 GB at maximum) up to 256 GB on large server \n",
      "machines. Large servers are like the ones you can get on cloud comput ing services \n",
      "such as Amazon Elastic Compute Cloud (EC2), whereas your  storage capabilities can \n",
      "easily exceed terabytes of capacity using just an external drive (most likely about 1 TB \n",
      "but it can reach up to 4 TB). As machine learning is based on globally r educing a cost \n",
      "function, many algorithms initially have been thought to work using all the available \n",
      "data and having access to it at each iteration of the optimization process  \n",
      "Can you name couple of ml challenges that you have faced?  \n",
      "Four main challenge s in Machine Learning include overfitting the data (using a model \n",
      "too complicated), underfitting the data (using a simple model), lacking in data and \n",
      "nonrepresentative data.  \n",
      "Can you please give 1 example of hyperparameter tuning wrt some classification  \n",
      "algorithm?  \n",
      "Machine learning algorithms have hyperparameters that allow you to tailor the \n",
      "behavior of the algorithm to your specific dataset.  \n",
      "Hyperparameters  are different from parameters, which are the internal coefficients or \n",
      "weights for a model found by the learning algorithm. Unlike parameters, \n",
      "hyperparameters are specified by the practitioner when configuring the model.  \n",
      "Typically, it is challenging to know what values to use for the hyperparameters of a \n",
      "given algorithm on a given dataset, therefore it is common to use random or grid search \n",
      "strategies for different hyperparameter values.  \n",
      "The more hyperparameters of an algorithm that you ne ed to tune, the slower the tuning \n",
      "process. Therefore, it is desirable to select a minimum subset of model \n",
      "hyperparameters to search or tune.  Not all model hyperparameters are equally \n",
      "important. Some hyperparameters have an outsized effect on the behavior, and in turn, \n",
      "the performance of a machine learning algorithm.  As a machine learning practitioner, \n",
      "you must know which hyperparameters to focus on to get a good result quickly.  \n",
      "Logistic Regression  \n",
      "Logistic regression does not really have any critical hyperp arameters to tune.  \n",
      "Sometimes, you can see useful differences in performance or convergence with \n",
      "different solvers ( solver ). \n",
      " solver  in [‘newton -cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’]  \n",
      "Regularization ( penalty ) can sometimes be helpful.  \n",
      " penalty  in [‘none’, ‘l1’, ‘l2’, ‘elasticnet’]  \n",
      "Note : not all solvers support all regularization terms.  \n",
      "The C parameter controls the penality strength, which can also be effective.  \n",
      " C in [100, 10, 1.0, 0.1, 0.01]  \n",
      "For the full list of hyperparameters, see:  \n",
      " sklearn.linear_model.LogisticRegression  API. \n",
      "The example below demonstrates grid searching the key hyperparameters for \n",
      "LogisticRegression on a synt hetic binary classification dataset.  \n",
      " \n",
      " \n",
      "What is out of bag evaluation?  What do you understand by hard & soft voting classifier?  \n",
      "\n",
      "In random forests, there is no need for cross -validation or a separate test set to get an \n",
      "unbiased estimate of the test set error. It is estimated internally, during the run, as \n",
      "follows:  \n",
      "Each tree is constructed using a different bootstrap sample from the  original data. \n",
      "About one -third of the cases are left out of the bootstrap sample and not used in the \n",
      "construction of the kth  tree.  \n",
      " \n",
      "Put each case left out in the construction of the kth tree down the kth tree to get a \n",
      "classification. In this way, a test set classification is obtained for each case in about \n",
      "one-third of the trees. At the end of the run, take j to be the class th at got most of the \n",
      "votes every time case n was oob. The proportion of times that j is not equal to the true \n",
      "class of n averaged over all cases is the oob error estimate. This has proven to be \n",
      "unbiased in many tests.  \n",
      "In classification, a hard voting ensembl e involves summing the votes for crisp class \n",
      "labels from other models and predicting the class with the most votes. A soft voting \n",
      "ensemble involves summing the predicted probabilities for class labels and predicting \n",
      "the class label with the largest sum pro bability.  \n",
      "Let’s Suppose I have trained 5 diff model with same training dataset & all of them have \n",
      "achieved 95%precision. Is there any chance that you can combine all these models to \n",
      "get better result? If yes, How? If no,  Why?  \n",
      "Yes, we can use K fold  \n",
      "Cross-validation is another method to estimate the skill of a method on unseen data. \n",
      "Like using a train -test split.Cross -validation systematically creates and evaluates \n",
      "multiple models on multiple subsets of the dataset.  This, in turn, provides a population \n",
      "of performance measures.  \n",
      "We can calculate the mean of these measures to get an idea of how well the procedure \n",
      "performs on average.  We can calculate the standard deviation of these measures to \n",
      "get an idea of how much the skill of the procedure is expected to  vary in practice.  \n",
      "This is also helpful for providing a more nuanced comparison of one procedure to \n",
      "another when you are trying to choose which algorithm and data preparation \n",
      "procedures to use.  Also, this information is invaluable as you can use the mean a nd \n",
      "spread to give a confidence interval on the expected performance on a machine \n",
      "learning procedure in practice.  Both train -test splits and k -fold cross validation are \n",
      "examples of resampling methods.  \n",
      "What do you understand by Gradient decent? How will you explain Gradient decent to \n",
      "a kid?  \n",
      "Gradient Descent is an optimization algorithm used for minimizing the cost function \n",
      "in various machine learning algorithms. It is basically used for updating the \n",
      "parameters of the learning model.  \n",
      "Types  of gradient  Descent:  \n",
      "1. Batch  Gradient  Descent:  This is a type of gradient descent which \n",
      "processes all the training examples for each iteration of gradient descent. \n",
      "But if the number of training examples is large, then batch gradient descent \n",
      "is computationally very expen sive. Hence if the number of training \n",
      "examples is large, then batch gradient descent is not preferred. Instead, \n",
      "we prefer to use stochastic gradient descent or mini -batch gradient \n",
      "descent.  \n",
      "2. Stochastic  Gradient  Descent:  This is a type of gradient descent whi ch \n",
      "processes 1 training example per iteration. Hence, the parameters are \n",
      "being updated even after one iteration in which only a single example has \n",
      "been processed. Hence this is quite faster than batch gradient descent. \n",
      "But again, when the number of trainin g examples is large, even then it \n",
      "processes only one example which can be additional overhead for the \n",
      "system as the number of iterations will be quite large.  \n",
      "3. Mini Batch  gradient  descent:  This is a type of gradient descent which works \n",
      "faster than both batch  gradient descent and stochastic gradient descent. \n",
      "Here  b examples where  b<m  are processed per iteration. So even if the \n",
      "number of training examples is large, it is processed in batches of b training \n",
      "examples in one go. Thus, it works for larger training e xamples and that \n",
      "too with lesser number of iterations.  \n",
      "An easy explanation is as follows:  \n",
      " \n",
      "Imagine that you were in the hills, and had to find the lowest valley.  \n",
      "Do this repeatedly:  \n",
      "Start from any point on any hill  \n",
      "Look in all four directions (ahead, behind, left, right) to determine where you might \n",
      "be able to descend (rather than ascend)  \n",
      "Take a step in that direction  \n",
      "Return to step (b) above  \n",
      "If you have reached a point where taking a step in any direction doesn’t make a \n",
      "difference, you’re at a minimum  (you’ve reached the valley)  \n",
      "Caveat: When you’re at a valley, from where you can see some other cavern or \n",
      "valley, you’re likely to be at a “local minimum”  \n",
      "Notes:  \n",
      " \n",
      "How big each step you take down the hills are - that represents your learning rate. \n",
      "Too big a  step, and you bounce between hills, and too small a step, and you take \n",
      "forever to descend into the valley  \n",
      "Gradient descent algorithms do much the same things, but in an arbitrary number of \n",
      "dimensions. The problem I’ve described above is in three dimension s of space, with \n",
      "steps taken over time. In mathematical functions, you can define arbitrarily large \n",
      "spaces where gradient descent can take place.  \n",
      " \n",
      "Can you please explain diff between regression & classification?  \n",
      "Classification and Regression are two major prediction problems which are usually \n",
      "dealt with Data mining and machine learning.   \n",
      "Classification  is the process of finding or discovering a model or function which helps \n",
      "in separating the data into multiple categorical classes i.e. discrete values.  In \n",
      "classification, data is categorized under different labels according to some \n",
      "parameters given in input and then the labels are predicted for the data.   \n",
      "The derived mapping function could be demonstrated in the form of “IF -THEN” rules. \n",
      "The classificatio n process deal with the problems where the data can be divided into \n",
      "binary or multiple discrete labels.   \n",
      "Let’s take an example, suppose we want to predict the possibility of the wining of \n",
      "match by Team A on the basis of some parameters recorded earlier. Th en there \n",
      "would be two labels Yes and No.   \n",
      "Regression  is the process of finding a model or function for distinguishing the data \n",
      "into continuous real values instead of using classes or discrete values. It can also \n",
      "identify the distribution movement depending  on the historical data. Because a \n",
      "regression predictive model predicts a quantity, therefore, the skill of the model must \n",
      "be reported as an error in those predictions   \n",
      "Let’s take a similar example in regression also, where we are finding the possibility \n",
      "of rain in some particular regions with the help of some parameters recorded earlier. \n",
      "Then there is a probability associated with the rain.   \n",
      "Explain a clustering algorithm of your choice.  \n",
      "K-Means Clustering  \n",
      "K-Means is by far the most popular clustering a lgorithm given that it is very easy to \n",
      "understand and apply to a wide range of data science and machine learning problems. \n",
      "Here’s how you can apply the K -Means algorithm to your clustering problem.  \n",
      "The first step is to select a number of clusters randomly,  each of which is represented \n",
      "by a variable ‘k’. Next, each cluster is assigned a centroid, i.e., the centre of that \n",
      "particular cluster. It is important to define the centroids as far off from each other as \n",
      "possible to reduce variation. After all the centr oids are defined, each data point is \n",
      "assigned to the cluster whose centroid is at the closest distance.  \n",
      "Once all data points are assigned to respective clusters, the centroid is again assigned \n",
      "for each cluster. Once again, all data points are rearranged i n specific clusters based \n",
      "on their distance from the newly defined centroids. This process is repeated until the \n",
      "centroids stop moving from their positions.  \n",
      "K-Means algorithm works wonders in grouping new data. Some of the practical \n",
      "applications of this a lgorithm are in sensor measurements, audio detection, and image \n",
      "segmentation.  \n",
      "How you can explain ML, DL, NLP, Computer vision & reinforcement learning with \n",
      "example in your own  term \n",
      " (AI) is the domain of producing intelligent machines. ML refers to systems that can \n",
      "assimilate from experience (training data) and Deep Learning (DL) states to \n",
      "systems that learn from experience on large data sets. ML can be considered as a \n",
      "subset of AI. Deep Learning (DL) is ML but useful to large data sets. The figure \n",
      "below roughly encapsulates the relation between AI, ML, and DL: In summary, DL \n",
      "is a subset of ML & both were the subsets of AI.  \n",
      "Additional Information: ASR (Automatic Speech Recognition) &  NLP (Natural \n",
      "Language Processing) fall under AI and overlay with ML & DL as ML is often utilized \n",
      "for NLP and ASR tasks.  \n",
      "NLP enables computers to understand natural language as humans do. Whether the \n",
      "language is spoken or written, natural language processing uses artificial intelligence \n",
      "to take real -world input, process it,  and make sense of it in a way a computer can \n",
      "understand. Just as humans have different sensors -- such as ears to hear and eyes \n",
      "to see -- computers have programs to read and microphones to collect audio. And just \n",
      "as humans have a brain to process that inp ut, computers have a program to process \n",
      "their respective inputs. At some point in processing, the input is converted to code that \n",
      "the computer can understand.  \n",
      "Computer vision:  \n",
      "Computer Vision, often abbreviated as CV, is defined as a field of study that se eks to \n",
      "develop techniques to help computers “see” and understand the content of digital \n",
      "images such as photographs and videos.The problem of computer vision appears \n",
      "simple because it is trivially solved by people, even very young children. Nevertheless, \n",
      "it largely remains an unsolved problem based both on the limited understanding of \n",
      "biological vision and because of the complexity of vision perception in a dynamic and \n",
      "nearly infinitely varying physical world.  \n",
      " \n",
      "Reinforcement Learning:  \n",
      "The model learns through a trial-and-error  method. This kind of learning involves an \n",
      "agent that will interact with the environment to create actions and then discover errors \n",
      "or rewards of that action.  \n",
      "Machine Learning involves algorithms that learn from pa tterns of data and then apply \n",
      "it to decision making. Deep Learning, on the other hand, is able to learn through \n",
      "processing data on its own and is quite similar to the human brain where it identifies \n",
      "something, analyse it, and makes a decision.  \n",
      "The key diff erences are as follow:  \n",
      "The manner in which data is presented to the system.  \n",
      "Machine learning algorithms always require structured data and deep learning \n",
      "networks rely on layers of artificial neural networks.  \n",
      " \n",
      "How you can explain semi -supervised ML in your own way with example?  \n",
      "Semi -supervised machine learning is a combination of supervised and unsupervised \n",
      "machine learning methods.  \n",
      " \n",
      "With more common supervised machine learning methods, you train a machine \n",
      "learning algorithm on a “labeled” dataset in wh ich each record includes the outcome \n",
      "information. This allows the algorithm to deduce patterns and identify relationships \n",
      "between your target variable and the rest of the dataset based on information it already \n",
      "has. In contrast, unsupervised machine learni ng algorithms learn from a dataset \n",
      "without the outcome variable. In semi -supervised learning, an algorithm learns from a \n",
      "dataset that includes both labeled and unlabeled data, usually mostly unlabeled.  \n",
      "Examples of semi supervised Ml  \n",
      "Speech Analysis: Since labeling of audio files is a very intensive task, Semi -\n",
      "Supervised learning is a very natural approach to solve this problem.  \n",
      "Internet Content Classification: Labeling each webpage is an impractical and \n",
      "unfeasible process and thus uses Semi -Supervised learn ing algorithms. Even the \n",
      "Google search algorithm uses a variant of Semi -Supervised learning to rank the \n",
      "relevance of a webpage for a given query.  \n",
      "Protein Sequence Classification: Since DNA strands are typically very large in size, \n",
      "the rise of Semi -Supervis ed learning has been imminent in this field.  \n",
      "What is difference between abstraction & generalization in your own word.  \n",
      "Abstraction is the process of removing details of objects. We step back from concrete \n",
      "objects to consider a number of objects with id entical properties. So a concrete object \n",
      "can be looked at as a “superset” of a more abstract object.  \n",
      " \n",
      "A generalization, then, is the formulation of general concepts from specific instances \n",
      "by abstracting common properties. A concrete object can be looked at as a “subset” of \n",
      "a more generalized object.  \n",
      "In other words:  \n",
      "1. For any two concepts A and B, A is an  abstraction  of B if and only if:  \n",
      " Every instance of concept B is also an instance of concept A  \n",
      "2. For any two concepts A and B, A is a  generalization  of B if a nd only if:  \n",
      " Every instance of concept B is also an instance of concept A  \n",
      " There are instances of concept A which are not instances of \n",
      "concept B  \n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Example Code:\n",
    "from PyPDF2 import PdfFileReader\n",
    "pdf_reader = PdfFileReader(r\"C:\\Users\\Gaurav Patil\\Downloads\\DataScience_interview_master_doc.pdf\")\n",
    "if pdf_reader.isEncrypted: # to check whether the pdf is encrypted or not\n",
    "    pdf_reader.decrypt(\"swordfish\")\n",
    "for page in pdf_reader.pages:\n",
    "    print(page.extractText()) # to print the text data of a page from pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5660e86e",
   "metadata": {},
   "source": [
    "### Question 5. What methods do you use to rotate a page?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "be0d1684",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'PyPDF2 Package provides 2 methods to rotate a page:\\n1. rotateClockWise() -> For Clockwise rotation\\n2. rotateCounterClockWise() -> For Counter Clockwise rotation\\nThe PyPDF2 package only allows to rotate a page in increments of 90 degrees, otherwise we receive an Assertion error'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''PyPDF2 Package provides 2 methods to rotate a page:\n",
    "1. rotateClockWise() -> For Clockwise rotation\n",
    "2. rotateCounterClockWise() -> For Counter Clockwise rotation\n",
    "The PyPDF2 package only allows to rotate a page in increments of 90 degrees, otherwise we receive an Assertion error'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011ff17d",
   "metadata": {},
   "source": [
    "### Question 6 . What is the difference between a Run object and a Paragraph object?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6fbaa388",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The structure of a document is represented by three different data types in python-Docx . At the highest level,\\na Document object represents the entire document. The Document object contains a list of Paragraph\\nobjects for the paragraphs in the document. (A new paragraph begins whenever the user presses ENTER or\\nRETURN while typing in a Word document.) Each of these Paragraph objects contain a list of one or more Run\\nobjects.\\nThe text in a Word document is more than just a string. It has font, size, color, and other styling information\\nassociated with it. A style in Word is a collection of these attributes. A Run object is a contiguous groups of\\ncharacters within a paragraph with the same style. A new Run object is needed whenever the text style changes.\\n'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The structure of a document is represented by three different data types in python-Docx . At the highest level,\n",
    "a Document object represents the entire document. The Document object contains a list of Paragraph\n",
    "objects for the paragraphs in the document. (A new paragraph begins whenever the user presses ENTER or\n",
    "RETURN while typing in a Word document.) Each of these Paragraph objects contain a list of one or more Run\n",
    "objects.\n",
    "The text in a Word document is more than just a string. It has font, size, color, and other styling information\n",
    "associated with it. A style in Word is a collection of these attributes. A Run object is a contiguous groups of\n",
    "characters within a paragraph with the same style. A new Run object is needed whenever the text style changes.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b4dfc5",
   "metadata": {},
   "source": [
    "### Question 7. How do you obtain a list of Paragraph objects for a Document object that’s stored in a variable named doc?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "17fc3e91",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [59]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example Program\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#!pip install python-docx (if docx module is not installed)\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      4\u001b[0m doc \u001b[38;5;241m=\u001b[39m Document(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_file.docx\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;66;03m# Path of the Docx file (NOTE: the extension may be .doc in case of a document file)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(doc\u001b[38;5;241m.\u001b[39mparagraphs) \u001b[38;5;66;03m# Prints the list of Paragraph objects for a Document\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     TAGS \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
     ]
    }
   ],
   "source": [
    "# Example Program\n",
    "#!pip install python-docx (if docx module is not installed)\n",
    "from docx import Document\n",
    "doc = Document(\"sample_file.docx\") # Path of the Docx file (NOTE: the extension may be .doc in case of a document file)\n",
    "print(doc.paragraphs) # Prints the list of Paragraph objects for a Document\n",
    "for paragraph in doc.paragraphs:\n",
    "    print(paragraph.text) # Prints the text in the paragraph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc237569",
   "metadata": {},
   "source": [
    "### Question 8. What type of object has bold, underline, italic, strike, and outline variables?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "466ddf46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A Run object has bold, underline, italic, strike, and outline variables.'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''A Run object has bold, underline, italic, strike, and outline variables.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc438df",
   "metadata": {},
   "source": [
    "### Question 9. What is the difference between False, True, and None for the bold variable?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6ea693d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Runs can be further styled using text attributes. Each attribute can be set to one of three values:\\n1. True (the attribute is always enabled, no matter what other styles are applied to the run, i.e. style Set to Bold)\\n2. False (the attribute is always disabled, i.e. Style Not Set to Bold)\\n3. None (defaults to whatever the run’s style is set to, i.e. Style is Not Applicable)\\nTrue always makes the Run object bolded and False makes it always not bolded, no matter what the style’s\\nbold setting is. None will make the Run object just use the style’s bold setting.\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Runs can be further styled using text attributes. Each attribute can be set to one of three values:\n",
    "1. True (the attribute is always enabled, no matter what other styles are applied to the run, i.e. style Set to Bold)\n",
    "2. False (the attribute is always disabled, i.e. Style Not Set to Bold)\n",
    "3. None (defaults to whatever the run’s style is set to, i.e. Style is Not Applicable)\n",
    "True always makes the Run object bolded and False makes it always not bolded, no matter what the style’s\n",
    "bold setting is. None will make the Run object just use the style’s bold setting.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b295f537",
   "metadata": {},
   "source": [
    "### Question 10. How do you create a Document object for a new Word document?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4059b714",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example Program\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      3\u001b[0m document \u001b[38;5;241m=\u001b[39m Document()\n\u001b[0;32m      4\u001b[0m document\u001b[38;5;241m.\u001b[39madd_paragraph(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miNeuron Full Stack DataScience Course\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     TAGS \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
     ]
    }
   ],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "document = Document()\n",
    "document.add_paragraph(\"iNeuron Full Stack DataScience Course\")\n",
    "document.save('myDocument.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4608958d",
   "metadata": {},
   "source": [
    "### Question 11. How do you add a paragraph with the text 'Hello, there!' to a Document object stored in a variable named doc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b844f4d0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'exceptions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [60]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example Program\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdocx\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Document\n\u001b[0;32m      3\u001b[0m doc \u001b[38;5;241m=\u001b[39m Document()\n\u001b[0;32m      4\u001b[0m doc\u001b[38;5;241m.\u001b[39madd_paragraph(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHello, there!\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\docx.py:30\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m     28\u001b[0m     TAGS \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;167;01mPendingDeprecationWarning\u001b[39;00m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m warn\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mlogging\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'exceptions'"
     ]
    }
   ],
   "source": [
    "# Example Program\n",
    "from docx import Document\n",
    "doc = Document()\n",
    "doc.add_paragraph('Hello, there!')\n",
    "doc.save('hello.docx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22741f19",
   "metadata": {},
   "source": [
    "### Question 12. What integers represent the levels of headings available in Word documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "48924ad2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The levels for a heading in a word document can be specified by using the level attribute inside the\\nadd_heading method. There are a total of 5 levels statring for 0 t0 4 , where level 0 makes a headline with\\nthe horizontal line below the text, whereas the heading level 1 is the main heading. Similarly, the other headings\\nare sub-heading with their's font-sizes in decreasing order.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''The levels for a heading in a word document can be specified by using the level attribute inside the\n",
    "add_heading method. There are a total of 5 levels statring for 0 t0 4 , where level 0 makes a headline with\n",
    "the horizontal line below the text, whereas the heading level 1 is the main heading. Similarly, the other headings\n",
    "are sub-heading with their's font-sizes in decreasing order.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ce2f0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
